{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OSM Merge, a Community Project of OpenStreetMap US","text":"<p>\ud83d\udcd6 Documentation: https://osm-merge.github.io/osm-merge/</p> <p>\ud83d\udda5\ufe0f Source Code: https://github.com/osm-merge/osm-merge</p>"},{"location":"#background","title":"Background","text":"<p>This is a project for conflating external map datasets with OpenStreetMap with the ultimate goal of importing it into OpenStreetMap. It is oriented towards processing non OSM external datasets and supports conflation of field data collection using OpenDataKit, as well as other external datasets.</p> <p>The goal of this project is focused on improving remote highway and trail metadata to assist with emergency and recreational access in remote areas. This project uses several data sources to improve the existing highway features in OpenStreetMap. The current data in OpenStreetMap was often imported complete with bugs in the  original dataset, or the only details are highway=track. All of these have a US forest service reference number and name. Adding those makes it much easier to identify where you are and to communicate a location over a radio or phone.</p> <p>There is also access information in the datasets that is useful. This includes access for vehicle types, horses, public/private, etc... that is useful for OpenStreetMap.</p> <p></p> <p>The other goal of this project is to support field data collection using OpenDataKit. The osm-fieldwork project can be used to convert the ODK data files into GeoJson and OSM XML. This project then supports conflating that field collected data with current OpenStreetMap. Otherwise this is a time-consuming process to do manually. This can be used with the Field Mapping Tasking Manager (FMTM) to process the collected data for validation and uploading to OpenStreetMap.</p> <p></p>"},{"location":"#external-datasets","title":"External Datasets","text":"<p>I'm working on a website for all the converted and processed data files that covers every national park or forest for the entire US. From the US forest service there are several datasets with a good license for OpenStreetMap. Warning, these are large files, so hard to work with at first. This project uses a huge amount of data (and disk space) if you start from the original nation wide datasets, which are too large to edit. There is a contrib script in the git sources I use to start breaking down the huge files into managable pieces.</p> <p>The MVUM data hasn't changed in years, so splitting everything up is a one-time task. For OpenStreetMap extracts all the data extracts of highways can be regenerated in a few hours, so it's possible to stay closely in sync with upstream. The original files are available from these sources.</p>"},{"location":"#us-forest-service-data","title":"US Forest Service data","text":"<ul> <li>Motor Vehicle Use Maps (MVUM)</li> <li>USFS Trail maps</li> </ul>"},{"location":"#us-park-service-data","title":"US Park Service data","text":"<ul> <li>NPS Trails</li> </ul>"},{"location":"#for-openstreetmap","title":"For OpenStreetMap","text":"<ul> <li>Geofabrik</li> </ul> <p>Much of the process of conflation is splitting huge datasets into managable sized files for data processing. I have that process mostly automated so I can easily regenerate data extracts any time I make improvements to the conversion process. Currently there isn't any conflated data yet, just the convered data files chopped into manageable sized files. The processed map data is available from here. Please note the website is work in progress. Feedback on the data conversion to OpenStreetMap tagging is appreciated.</p> <p></p>"},{"location":"#programs","title":"Programs","text":"<p>This project comprises of a main program, and multiple utilities. The utilities are used to prepare the datasets for conflating by fixing known bugs. It is very difficult to conflate datasets with wildly different data schemas, so making all the data use a consistent schema is important so tags can be compared.</p>"},{"location":"#utilities","title":"Utilities","text":"<p>The utility programs are used for all data cleaning and other tasks needed to conflate the data files. Known bugs in the datasets are fixed where possible, for example, expanding abbreviations so Rd becomes Road, etc... and also drops all the extraneous fields that aren't for OpenStreetMap. The fields we are most interested in are the name, the official reference number, and the access values.</p> <ul> <li>tm-splitter.py<ul> <li>Generate task grids for the Tasking Manager</li> </ul> </li> <li>mvum.py<ul> <li>Convert forest service MVUM datasets to OpenStreetMap</li> </ul> </li> <li>trails.py<ul> <li>Convert forest service datasets to OpenStreetMap</li> </ul> </li> <li>usgs.py<ul> <li>Convert USGS topographical datasets to OpenStreetMap</li> </ul> </li> <li>osmhighways.py<ul> <li>Data janitor for OpenStreetMap, delete tiger:, etc...</li> </ul> </li> </ul>"},{"location":"#conflator-program","title":"Conflator Program","text":"<p>This program doesn't require a database, unlike the other conflation programs in this project, although adding database support is on the TODO list. It is focused on conflating rural highways and hiking trails. It can also conflate OpenDataKit with OpenStreetMap.</p> <p>It supports conflating any two datasets in either GeoJson or OSM format. While this is currently under heavy development and debugging. I've been processing large amounts of data to track down all the obscure bugs in the original datasets, or the conflation process.</p> <p></p>"},{"location":"#contributing","title":"Contributing","text":"<p>Anyone motivated is welcome to contribute to both the software, or just using these tools and the data for your own map improvements. This is a huge amount of data, as the original source files are nationwide. I'm just focused on my part of the western US. Help improving OpenStreetMap accuracy in remote areas of your own state is a good idea, and can save lives in an emergency. Be a data janitor!</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"LICENSE/","title":"GNU AFFERO GENERAL PUBLIC LICENSE","text":"<p>Version 3, 19 November 2007</p> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"LICENSE/#preamble","title":"Preamble","text":"<p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"LICENSE/#terms-and-conditions","title":"TERMS AND CONDITIONS","text":""},{"location":"LICENSE/#0-definitions","title":"0. Definitions.","text":"<p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p>"},{"location":"LICENSE/#1-source-code","title":"1. Source Code.","text":"<p>The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p>"},{"location":"LICENSE/#2-basic-permissions","title":"2. Basic Permissions.","text":"<p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.</p>"},{"location":"LICENSE/#3-protecting-users-legal-rights-from-anti-circumvention-law","title":"3. Protecting Users' Legal Rights From Anti-Circumvention Law.","text":"<p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p>"},{"location":"LICENSE/#4-conveying-verbatim-copies","title":"4. Conveying Verbatim Copies.","text":"<p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p>"},{"location":"LICENSE/#5-conveying-modified-source-versions","title":"5. Conveying Modified Source Versions.","text":"<p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <ul> <li>a) The work must carry prominent notices stating that you modified     it, and giving a relevant date.</li> <li>b) The work must carry prominent notices stating that it is     released under this License and any conditions added under     section 7. This requirement modifies the requirement in section 4     to \"keep intact all notices\".</li> <li>c) You must license the entire work, as a whole, under this     License to anyone who comes into possession of a copy. This     License will therefore apply, along with any applicable section 7     additional terms, to the whole of the work, and all its parts,     regardless of how they are packaged. This License gives no     permission to license the work in any other way, but it does not     invalidate such permission if you have separately received it.</li> <li>d) If the work has interactive user interfaces, each must display     Appropriate Legal Notices; however, if the Program has interactive     interfaces that do not display Appropriate Legal Notices, your     work need not make them do so.</li> </ul> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p>"},{"location":"LICENSE/#6-conveying-non-source-forms","title":"6. Conveying Non-Source Forms.","text":"<p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <ul> <li>a) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by the     Corresponding Source fixed on a durable physical medium     customarily used for software interchange.</li> <li>b) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by a     written offer, valid for at least three years and valid for as     long as you offer spare parts or customer support for that product     model, to give anyone who possesses the object code either (1) a     copy of the Corresponding Source for all the software in the     product that is covered by this License, on a durable physical     medium customarily used for software interchange, for a price no     more than your reasonable cost of physically performing this     conveying of source, or (2) access to copy the Corresponding     Source from a network server at no charge.</li> <li>c) Convey individual copies of the object code with a copy of the     written offer to provide the Corresponding Source. This     alternative is allowed only occasionally and noncommercially, and     only if you received the object code with such an offer, in accord     with subsection 6b.</li> <li>d) Convey the object code by offering access from a designated     place (gratis or for a charge), and offer equivalent access to the     Corresponding Source in the same way through the same place at no     further charge. You need not require recipients to copy the     Corresponding Source along with the object code. If the place to     copy the object code is a network server, the Corresponding Source     may be on a different server (operated by you or a third party)     that supports equivalent copying facilities, provided you maintain     clear directions next to the object code saying where to find the     Corresponding Source. Regardless of what server hosts the     Corresponding Source, you remain obligated to ensure that it is     available for as long as needed to satisfy these requirements.</li> <li>e) Convey the object code using peer-to-peer transmission,     provided you inform other peers where the object code and     Corresponding Source of the work are being offered to the general     public at no charge under subsection 6d.</li> </ul> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p>"},{"location":"LICENSE/#7-additional-terms","title":"7. Additional Terms.","text":"<p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <ul> <li>a) Disclaiming warranty or limiting liability differently from the     terms of sections 15 and 16 of this License; or</li> <li>b) Requiring preservation of specified reasonable legal notices or     author attributions in that material or in the Appropriate Legal     Notices displayed by works containing it; or</li> <li>c) Prohibiting misrepresentation of the origin of that material,     or requiring that modified versions of such material be marked in     reasonable ways as different from the original version; or</li> <li>d) Limiting the use for publicity purposes of names of licensors     or authors of the material; or</li> <li>e) Declining to grant rights under trademark law for use of some     trade names, trademarks, or service marks; or</li> <li>f) Requiring indemnification of licensors and authors of that     material by anyone who conveys the material (or modified versions     of it) with contractual assumptions of liability to the recipient,     for any liability that these contractual assumptions directly     impose on those licensors and authors.</li> </ul> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p>"},{"location":"LICENSE/#8-termination","title":"8. Termination.","text":"<p>You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p>"},{"location":"LICENSE/#9-acceptance-not-required-for-having-copies","title":"9. Acceptance Not Required for Having Copies.","text":"<p>You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p>"},{"location":"LICENSE/#10-automatic-licensing-of-downstream-recipients","title":"10. Automatic Licensing of Downstream Recipients.","text":"<p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p>"},{"location":"LICENSE/#11-patents","title":"11. Patents.","text":"<p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p>"},{"location":"LICENSE/#12-no-surrender-of-others-freedom","title":"12. No Surrender of Others' Freedom.","text":"<p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p>"},{"location":"LICENSE/#13-remote-network-interaction-use-with-the-gnu-general-public-license","title":"13. Remote Network Interaction; Use with the GNU General Public License.","text":"<p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p>"},{"location":"LICENSE/#14-revised-versions-of-this-license","title":"14. Revised Versions of this License.","text":"<p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p>"},{"location":"LICENSE/#15-disclaimer-of-warranty","title":"15. Disclaimer of Warranty.","text":"<p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p>"},{"location":"LICENSE/#16-limitation-of-liability","title":"16. Limitation of Liability.","text":"<p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"LICENSE/#17-interpretation-of-sections-15-and-16","title":"17. Interpretation of Sections 15 and 16.","text":"<p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"LICENSE/#how-to-apply-these-terms-to-your-new-programs","title":"How to Apply These Terms to Your New Programs","text":"<p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"about/","title":"Conflator","text":"<p>This is a project for conflating map data, with the ultimate goal of importing it into OpenStreetMap(OSM).</p> <p>It is oriented towards conflating external datasets with existing OSM data. External data is usually polygons (building footprints), or POIs. These days there are multiple publically available building footprint datasets with an appropriate license for OSM. The problem is this data needs to be validated.</p> <p>Due to the flexibility of the OSM data schema, it's impossible to get 100% perfect conflation. But the purely manual conflation is very time consuming and tedious. This project aims to do as much as possible to aid the validator to make their work as efficient as possible.</p>"},{"location":"api/","title":"API Docs for conflator","text":""},{"location":"api/#programs","title":"Programs","text":""},{"location":"api/#utilities","title":"Utilities","text":"<p>               Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>dataspec</code> <code>str</code> <p>The input data to convert</p> <code>None</code> <code>yamlspec</code> <code>str</code> <p>The YAML config file for converting data</p> <code>'utilities/mvum.yaml'</code> <p>Returns:</p> Type Description <code>MVUM</code> <p>An instance of this class</p> Source code in <code>osm_merge/utilities/mvum.py</code> <pre><code>def __init__(self,\n             dataspec: str = None,\n             yamlspec: str = \"utilities/mvum.yaml\",\n             ):\n    \"\"\"\n    This class processes the MVUM dataset.\n\n    Args:\n        dataspec (str): The input data to convert\n        yamlspec (str): The YAML config file for converting data\n\n    Returns:\n        (MVUM): An instance of this class\n    \"\"\"\n    self.file = None\n    if dataspec is not None:\n        self.file = open(dataspec, \"r\")\n\n    yaml = f\"{rootdir}/{yamlspec}\"\n    if not os.path.exists(yaml):\n        log.error(f\"{yaml} does not exist!\")\n        quit()\n\n    file = open(yaml, \"r\")\n    self.yaml = YamlFile(f\"{yaml}\")\n</code></pre> <p>options: show_source: false heading_level: 3</p> <p>               Bases: <code>object</code></p> Source code in <code>osm_merge/utilities/usgs.py</code> <pre><code>def __init__(self,\n             filespec: str = None,\n             ):\n    self.file = None\n    if filespec is not None:\n        self.file = open(filespec, \"r\")\n</code></pre> <p>options: show_source: false heading_level: 3</p> <p>options: show_source: false heading_level: 3</p>"},{"location":"api/#osm_merge.utilities.usgs.USGS.convert","title":"convert","text":"<pre><code>convert(state, filespec=None)\n</code></pre> <p>Convert the USGS topographical dataset to something that can be conflated. The dataset schema is pretty ugly, duplicate field names, abbreviations, etc... plus a shapefile truncates the field names.</p> <p>Parameters:</p> Name Type Description Default <code>filespec</code> <code>str</code> <p>The input dataset file</p> <code>None</code> <code>state</code> <code>str</code> <p>The 2 letter state abbreviation</p> required Source code in <code>osm_merge/utilities/usgs.py</code> <pre><code>def convert(self,\n            state: str,\n            filespec: str = None,\n            ) -&gt; list:\n    \"\"\"\n    Convert the USGS topographical dataset to something that can\n    be conflated. The dataset schema is pretty ugly, duplicate\n    field names, abbreviations, etc... plus a shapefile truncates\n    the field names.\n\n    Args:\n        filespec (str): The input dataset file\n        state (str): The 2 letter state abbreviation\n\n    \"\"\"\n    # FIXME: read in the whole file for now\n    if filespec is not None:\n        file = open(filespec, \"r\")\n    else:\n        file = self.file\n\n    data = geojson.load(file)\n\n    highways = list()\n    spin = Bar('Processing...', max=len(data['features']))\n    for entry in data[\"features\"]:\n        geom = entry[\"geometry\"]\n        props = dict()\n        spin.next()\n        if \"name\" in entry[\"properties\"]:\n            props[\"name\"] =  entry[\"properties\"][\"name\"]\n        if \"sourceorig\" in entry[\"properties\"]:\n            props[\"highway\"] = \"path\"\n            # These are for the trail data\n            if entry['properties']['sourceorig'] is not None:\n                props[\"source\"] = entry['properties']['sourceorig']\n            if 'trailnumbe' in entry['properties']:\n                if entry['properties']['trailnumbe'] is not None:\n                    props[\"ref:usfs\"] = f\"{entry['properties']['trailnumbe']}\"\n            if \"bicycle\" in entry['properties']:\n                if entry['properties']['bicycle'] is not None:\n                    if entry['properties']['bicycle'] == \"Y\":\n                        props[\"bicycle\"] = \"designated\"\n                    # else:\n                    #     props[\"bicycle\"] = \"no\"\n            if \"atv\" in entry['properties']:\n                if entry['properties']['atv'] is not None:\n                    if entry['properties']['atv'] == \"Y\":\n                        props[\"atv\"] = \"designated\"\n                    # else:\n                    #     props[\"atv\"] = \"no\"\n            if \"packsaddle\" in entry['properties']:\n                if entry['properties']['packsaddle'] is not None:\n                    if entry['properties']['packsaddle'] == \"Y\":\n                        props[\"horse\"] = \"designated\"\n                    # else:\n                    #     props[\"horse\"] = \"no\"\n            if \"motorcycle\" in entry['properties']:\n                if entry['properties']['motorcycle'] is not None:\n                    if entry['properties']['motorcycle'] == \"Y\":\n                        props[\"motorcycle\"] = \"designated\"\n                    # else:\n                    #     props[\"motorcycle\"] = \"no\"\n            if 'snowshoe' in entry['properties']:\n                if entry['properties']['snowshoe'] is not None:\n                    if entry['properties']['snowshoe'] == \"Y\":\n                        props[\"piste:type\"] = \"hike\"\n                if entry['properties']['crosscount'] is not None:\n                    if entry['properties']['crosscount'] == \"Y\":\n                        props[\"ski\"] = \"yes\"\n                        props[\"piste:type\"] = \"nordic\"\n            # if entry['properties']['dogsled'] is not None:\n            #     if entry['properties']['dogsled'] == \"Y\":\n            #         props[\"dogsled\"] = \"yes\"\n            # if entry['properties']['hikerpedes'] is not None:\n            #     if entry['properties']['hikerpedes'] == \"Y\":\n            #         props[\"hiker\"] = \"yes\"\n            #     else:\n            #         props[\"hiker\"] = \"no\"\n            if \"snowmobile\"in entry['properties']:\n                if entry['properties']['snowmobile'] is not None:\n                    if entry['properties']['snowmobile'] == \"Y\":\n                        props[\"snowmobile\"] = \"designated\"\n                    # else:\n                    #     props[\"snowmobile\"] = \"no\"\n            if \"motorizedw\" in entry['properties']:\n                if entry['properties']['motorizedw'] is not None:\n                    # FIXME: there's a better tag\n                    if entry['properties']['motorizedw'] == \"Y\":\n                        props[\"motorized\"] = \"designated\"\n                    # else:\n                    #     props[\"motorized\"] = \"no\"\n            if len(props) == 0:\n                continue\n            if geom is not None:\n                highways.append(Feature(geometry=geom, properties=props))\n            continue\n\n        # These are for the highways data\n        if \"highway\" not in entry[\"properties\"]:\n            props[\"highway\"] = \"unclassified\"\n        if entry[\"properties\"] is None or entry is None:\n            continue\n        if 'source_ori' in entry['properties']:\n            if entry['properties']['source_ori'] is not None:\n                props[\"source\"] = entry['properties']['source_ori']\n        if 'us_route_a' in entry['properties']:\n            if entry['properties']['us_route_a'] is not None:\n                props[\"ref\"] = f\"US {entry['properties']['us_route_a']}\"\n        if 'us_route' in entry['properties']:\n            if entry['properties']['us_route'] is not None:\n                props[\"ref\"] = f\"US {entry['properties']['us_route']}\"\n        if 'county_rou' in entry['properties']:\n            if entry['properties']['county_rou'] is not None:\n                props[\"ref\"] = f\"US {entry['properties']['county_rou']}\"\n        if 'state_ro_1' in entry['properties']:\n            if entry['properties']['state_ro_1'] is not None:\n                props[\"ref\"] = f\"{state} {entry['properties']['state_ro_1']}\"\n        if 'state_rout' in entry['properties']:\n            if entry['properties']['state_rout'] is not None:\n                props[\"ref\"] = f\"{state} {entry['properties']['state_rout']}\"\n        if 'federal_la' in entry['properties']:\n            if entry['properties']['federal_la'] is not None:\n                # FIXME: add . between numbers and Letters.\n                props[\"ref:usfs\"] = f\"FR {entry['properties']['federal_la']}\"\n\n        if 'name' not in entry['properties']:\n            continue\n        if  entry['properties']['name'] is not None:\n            if entry['properties']['name'][:8] == \"USFS Rd \":\n                props[\"ref:usfs\"] = f\"FR {entry['properties']['name'][8:]}\"\n            elif entry['properties']['name'][:3] == \"Rd \":\n                props[\"ref\"] = f\"CR {entry['properties']['name'][3:]}\"\n                props[\"name\"] = f\"County Road {entry['properties']['name'][3:]}\"\n            elif entry['properties']['name'][:6] == \"Co Rd \":\n                props[\"ref\"] = f\"CR {entry['properties']['name'][6:]}\"\n                props[\"name\"] = f\"County Road {entry['properties']['name'][6:]}\"\n            elif entry['properties']['name'][:6] == \"State Hwy \":\n                props[\"ref\"] = f\"{state} {entry['properties']['name'][6:]}\"\n                props[\"name\"] = f\"State Highway {entry['properties']['name'][6:]}\"\n            elif entry['properties']['name'][:6] == \"Us Hwy \":\n                props[\"ref\"] = f\"US {entry['properties']['name'][6:]}\"\n                props[\"name\"] = f\"US Highway {entry['properties']['name'][6:]}\"\n            else:\n                # The USGS topo data when it comes to names is a real\n                # mess, full of abbreviations. So expand them which\n                # is what OSM prefers, and of course will be needed\n                # when conflating to get a string match.\n                name = f\"{entry['properties']['name'].title()}\"\n                name = name.replace(\" Rd\", \" Road\")\n                name = name.replace(\" Hwy\", \" Highway\")\n                name = name.replace(\" Ln\", \" Lane\")\n                name = name.replace(\" Mnt\", \" Mountain \")\n                name = name.replace(\"E \", \"East \")\n                name = name.replace(\"W \", \"West \")\n                name = name.replace(\"N \", \"North \")\n                name = name.replace(\"S \", \"South \")\n                props[\"name\"] = name\n\n        if len(props) == 0 or geom is None:\n            continue\n        highways.append(Feature(geometry=geom, properties=props))\n\n    return FeatureCollection(highways)\n</code></pre>"},{"location":"api/#osm_merge.utilities.osmhighways.getRef","title":"getRef","text":"<pre><code>getRef(name)\n</code></pre> <p>Extract the reference number in the name string</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the highway</p> required <p>Returns:</p> Type Description <code>str</code> <p>The reference number, which is an alphanumeric</p> Source code in <code>osm_merge/utilities/osmhighways.py</code> <pre><code>def getRef(name) -&gt; str:\n    \"\"\"\n    Extract the reference number in the name string\n\n    Args:\n        name (str): The name of the highway\n\n    Returns:\n        (str): The reference number, which is an alphanumeric\n    \"\"\"\n    if not name:\n        # log.error(f\"Nothing done, empty string\")\n        return name\n\n    # The number is always (supposedly) last, so this gets all the\n    # weird contortions without getting complicated.\n    ref = \"[0-9].*+\"\n    pat = re.compile(ref)\n    result = pat.findall(name.lower())\n    if len(result) == 0:\n        # if it's as a an integer, not decimals or character appended,\n        # look for that too.\n        ref = \" [0-9]+\"\n        pat = re.compile(ref)\n        result = pat.findall(name.lower())\n        if len(result) == 0:\n            return name\n        else:\n            return result[0].strip().replace(' ', '.')\n    else:\n        if '/' in result[0]:\n            return result[0]\n        else:\n            return result[0].replace(' ', '.')\n</code></pre>"},{"location":"api/#osm_merge.utilities.osmhighways.filterTags","title":"filterTags","text":"<pre><code>filterTags(obj)\n</code></pre> <p>Filter the tags, which entails fixing typos, abbreviations, etc...</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>The feature to filter</p> required <p>Returns:</p> Source code in <code>osm_merge/utilities/osmhighways.py</code> <pre><code>def filterTags(obj):\n    \"\"\"\n    Filter the tags, which entails fixing typos, abbreviations, etc...\n\n    Args:\n        obj: The feature to filter\n\n    Returns:\n\n    \"\"\"\n    fix = [\"name\", \"ref\", \"ref:usfs\"]\n\n    name = None\n    newtags = dict() # obj.tags\n    if \"name\" in obj.tags:\n        name = obj.tags.get(\"name\")\n    # log.debug(f\"NAME: {name}\")\n    for tag in obj.tags:\n        matched = False\n        key = tag[0]\n        val = tag[1]\n        # The OSM community has long ago decided these tags from the\n        # TIGER import are useless, and should be deleted.\n        if key[:6] == \"tiger:\":\n            continue\n        # Here's another import mess. The original MVUM data got imported\n        # but all the original fields got add, often over a dozen. They\n        # luckily all start with a _ character followed by an upper case\n        # field name. OSM tags are always lower case. Delete all these tags.\n        # Anything interesting like HIGH_CLEARANCE_VEHICLE=YES will\n        # get added during conflation, so these fields aren't needed.\n        pat = re.compile(\"^_[A-Z]+\")\n        if pat.match(key):\n            continue\n        if key not in fix:\n            newtags[key] = val\n            continue\n\n        # It's the name tag that has the most problems.\n        if key == \"ref\" or key == \"ref:usfs\":\n            # A good ref has an FR or FS prefixed, so just use it, but move it\n            # to the ref:usfs tag.\n            if val[:3] == \"FS \" or val[:3] == \"FR \":\n                newtags[\"ref:usfs\"] = val\n                continue\n            elif val[:4] == \"FSR \":\n                ref = getRef(val)\n                newtags[\"ref:usfs\"] = f\"FR {ref}\"\n                continue\n            elif key == \"ref\" and val[:3] == \"CR \":\n                # It's a well mapped county road, do nothing\n                newtags[key] = val\n                continue\n            ref = getRef(name)\n            if ref and len(ref) &gt; 0:\n                # log.debug(f\"MATCHED: {pat.pattern}\")\n                newtags[\"ref:usfs\"] = f\"FR {ref}\"\n            matched = True\n            continue\n\n        usfspats = [\"fire road\",\n                    \"fs.* road\",\n                    \"f[sd]r \",\n                    \"usfsr \",\n                    \"fs[hr] \",\n                    \"usf.* road\",\n                    \"national forest road\",\n                    \"forest service road\",\n                    \"fr \",\n                    \"fs \",\n                    \"forest road\",\n                    \"usfs trail \",\n                    ]\n        if key == \"name\" and name is not None:\n            pat = re.compile(\"county road\")\n            if pat.match(name.lower()):\n                for entry in name.split(';'):\n                    ref = getRef(entry)\n                    # log.debug(f\"COUNTY: {pat.pattern} REF={ref.title()} NAME={name}\")\n                    if ref and len(ref) &gt; 0:\n                        newtags[\"ref\"] = f\"CR {ref.title()}\"\n                    matched = True\n                continue\n\n            # FIXME: Since we're focused on roads in national forests or\n            # parks, there shouldn't be any state or federal highways,\n            # but you never know, this should be confirmed.\n            # pat = re.compile(\"state highway\")\n            # pat = re.compile(\"united states highway\")\n\n            for regex in usfspats:\n                pat = re.compile(regex)\n                if pat.match(name.lower()):\n                    for entry in name.split(';'):\n                        ref = getRef(entry)\n                        # log.debug(f\"MATCHED: {pat.pattern} REF={ref.title()} NAME={name}\")\n                        if ref and len(ref) &gt; 0:\n                            newtags[\"ref:usfs\"] = f\"FR {ref.title()}\"\n                    matched = True\n                    break\n            if not matched:\n                newtags[key] = val\n        else:\n            newtags[key] = val\n\n    # print(f\"OLDTAGS: {len(obj.tags)}: {obj.tags}\")\n    # print(f\"NEWTAGS: {len(newtags)} {newtags}\")\n    return newtags\n</code></pre>"},{"location":"api/#osm_merge.utilities.osmhighways.clip","title":"clip","text":"<pre><code>clip(boundary, infile, outfile)\n</code></pre> <p>Clip the data in a file by a multipolygon instead of using the command line program. The output file will only contain ways, in JOSM doing \"File-&gt;update data' will load all the nodes so the highways are visible. It's slower of course than the C++ version, but this gives us better fine-grained control.</p> <p>Parameters:</p> Name Type Description Default <code>infile</code> <code>str</code> <p>The input data</p> required <code>outfile</code> <code>str</code> <p>The output file</p> required <code>boundary</code> <code>str</code> <p>The boundary</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether it worked or not</p> Source code in <code>osm_merge/utilities/osmhighways.py</code> <pre><code>def clip(boundary: str,\n         infile: str,\n         outfile: str,\n         ):\n    \"\"\"\n    Clip the data in a file by a multipolygon instead of using\n    the command line program. The output file will only contain\n    ways, in JOSM doing \"File-&gt;update data' will load all the\n    nodes so the highways are visible. It's slower of course\n    than the C++ version, but this gives us better fine-grained\n    control.\n\n    Args:\n        infile (str): The input data\n        outfile (str): The output file\n        boundary (str): The boundary\n\n    Returns:\n        (bool): Whether it worked or not\n    \"\"\"\n    timer = Timer(text=\"clip() took {seconds:.0f}s\")\n    timer.start()\n\n    # Load the boundary\n    file = open(boundary, 'r')\n    data = geojson.load(file)\n    boundary = data[\"features\"]\n    task = shape(boundary[0][\"geometry\"])\n\n    if os.path.exists(outfile):\n        os.remove(outfile)\n    nodes = set()\n    # Pre-filter the ways by tags. The less object we need to look at, the better.\n    way_filter = osmium.filter.KeyFilter('highway')\n    # only scan the ways of the file\n    spin = Spinner('Processing nodes...')\n    fp = osmium.FileProcessor(infile, osmium.osm.WAY).with_filter(osmium.filter.KeyFilter('highway'))\n    for obj in fp:\n       spin.next()\n       if \"highway\" in obj.tags:\n           nodes.update(n.ref for n in obj.nodes)\n\n    writer = osmium.SimpleWriter(outfile)\n\n    # We need nodes and ways in the second pass.\n    fab = GeoJSONFactory()\n    spin = Spinner('Processing ways...')\n    way_filter = osmium.filter.KeyFilter('highway').enable_for(osmium.osm.WAY)\n    for obj in osmium.FileProcessor(infile, osmium.osm.WAY | osmium.osm.NODE).with_filter(way_filter).with_locations():\n        spin.next()\n        if obj.is_node() and obj.id in nodes:\n            # We don't want POIs for barrier or crossing, just LineStrings\n            if len(obj.tags) &gt; 0:\n                continue\n            wkt = fab.create_point(obj)\n            geom = shape(geojson.loads(wkt))\n            # Add a node if it exists within the boundary\n            if contains(task, geom) or intersects(task, geom):\n                # writer.add(obj)\n                # log.debug(f\"Adding {obj.id}\")\n                continue\n                # Strip the object of tags along the way\n            # writer.add_node(obj.replace(tags={}))\n        elif obj.is_way() and \"highway\" in obj.tags:\n            wkt = fab.create_linestring(obj.nodes)\n            geom = shape(geojson.loads(wkt))\n            if contains(task, geom) or intersection(task, geom):\n                writer.add_way(obj)\n    timer.stop()\n    return True\n</code></pre>"},{"location":"api/#osm_merge.utilities.osmhighways.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>This main function lets this class be run standalone by a bash script</p> Source code in <code>osm_merge/utilities/osmhighways.py</code> <pre><code>def main():\n    \"\"\"This main function lets this class be run standalone by a bash script\"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"usgs\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=\"This program extracts highways from OSM\",\n        epilog=\"\"\"\nThis program extracts all the highways from an OSM file, and correct as\nmany of the bugs with names that are actually a reference number. \n\n    For Example: \n        osmhighways.py -v -i colorado-latest.osm.pbf -o co-highways.osm\n        \"\"\",\n    )\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"verbose output\")\n    parser.add_argument(\"-i\", \"--infile\", required=True, help=\"Top-level input directory\")\n    parser.add_argument(\"-o\", \"--outfile\", default=\"out.osm\", help=\"Output file\")\n    parser.add_argument(\"-c\", \"--clip\", help=\"Clip file by polygon\")\n    parser.add_argument(\"-s\", \"--small\", help=\"Small dataset\")\n\n    args = parser.parse_args()\n    # if verbose, dump to the terminal.\n    log_level = os.getenv(\"LOG_LEVEL\", default=\"INFO\")\n    if args.verbose is not None:\n        log_level = logging.DEBUG\n\n    logging.basicConfig(\n        level=log_level,\n        format=(\"%(asctime)s.%(msecs)03d [%(levelname)s] \" \"%(name)s | %(funcName)s:%(lineno)d | %(message)s\"),\n        datefmt=\"%y-%m-%d %H:%M:%S\",\n        stream=sys.stdout,\n    )\n\n    if args.clip:\n        # cachefile = os.path.basename(args.infile.replace(\".pbf\", \".cache\"))\n        # create_nodecache(args.infile, cachefile)\n        if not clip:\n            log.error(f\"You must specify a boundary!\")\n            parser.print_help()\n            quit()\n        if not args.infile:\n            log.error(f\"You must specify the input file!\")\n            parser.print_help()\n            quit()\n\n        clip(args.clip, args.infile, args.outfile)\n        log.info(f\"Wrote clipped file {args.outfile}\")\n        quit()\n\n    # FIXME: this should change\n    outfile = args.outfile\n    keep = [\"track\",\n            \"unclassified\",\n            \"residential\",\n            \"path\",\n            \"footway\",\n            \"pedestrian\"\n            \"primary\",\n            \"secondary\",\n            \"tertiary\",\n            \"trunk\",\n            \"motorway\",\n            ]\n    spin = Spinner('Processing...')\n    fp = osmium.FileProcessor(args.infile).with_filter(osmium.filter.KeyFilter('highway'))\n    with osmium.BackReferenceWriter(outfile, ref_src=args.infile, overwrite=True) as writer:\n        for obj in fp:\n            spin.next()\n            if obj.tags['highway'] in keep and obj.is_way():\n                tags = filterTags(obj)\n                writer.add(obj.replace(tags=tags))\n    log.info(f\"Wrote {outfile}\")\n</code></pre>"},{"location":"api/#geosupportpy","title":"geosupport.py","text":"<p>               Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>dburi</code> <code>str</code> <p>The database URI</p> <code>None</code> <code>config</code> <code>str</code> <p>The config file from the osm-rawdata project</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoSupport</code> <p>An instance of this object</p> Source code in <code>osm_merge/geosupport.py</code> <pre><code>def __init__(self,\n             dburi: str = None,\n             config: str = None,\n             ):\n    \"\"\"\n    This class conflates data that has been imported into a postgres\n    database using the Underpass raw data schema.\n\n    Args:\n        dburi (str, optional): The database URI\n        config (str, optional): The config file from the osm-rawdata project\n\n    Returns:\n        (GeoSupport): An instance of this object\n    \"\"\"\n    self.db = None\n    self.dburi = dburi\n    self.config = config\n</code></pre> <p>options: show_source: false heading_level: 3</p>"},{"location":"api/#osm_merge.geosupport.GeoSupport.importDataset","title":"importDataset  <code>async</code>","text":"<pre><code>importDataset(filespec)\n</code></pre> <p>Import a GeoJson file into a postgres database for conflation.</p> <p>Parameters:</p> Name Type Description Default <code>filespec</code> <code>str</code> <p>The GeoJson file to import</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the import was successful</p> Source code in <code>osm_merge/geosupport.py</code> <pre><code>async def importDataset(self,\n                 filespec: str,\n                 ) -&gt; bool:\n    \"\"\"\n    Import a GeoJson file into a postgres database for conflation.\n\n    Args:\n        filespec (str): The GeoJson file to import\n\n    Returns:\n        (bool): If the import was successful\n    \"\"\"\n    file = open(filespec, \"r\")\n    data = geojson.load(file)\n\n    # Create the tables\n    sql = \"CREATE EXTENSION postgis;\"\n    result = await self.db.execute(sql)\n    sql = f\"DROP TABLE IF EXISTS public.nodes CASCADE; CREATE TABLE public.nodes (osm_id bigint, geom geometry, tags jsonb);\"\n    result = await self.db.execute(sql)\n    sql = f\"DROP TABLE IF EXISTS public.ways_line CASCADE; CREATE TABLE public.ways_line (osm_id bigint, geom geometry, tags jsonb);\"\n    result = await self.db.execute(sql)\n    sql = f\"DROP TABLE IF EXISTS public.poly CASCADE; CREATE TABLE public.ways_poly (osm_id bigint, geom geometry, tags jsonb);\"\n    result = await self.db.execute(sql)\n\n    # if self.db.is_closed():\n    #     return False\n\n    table = self.dburi.split('/')[1]\n    for entry in data[\"features\"]:\n        keys = \"geom, \"\n        geometry = shape(entry[\"geometry\"])\n        ewkt = geometry.wkt\n        if geometry.geom_type == \"LineString\":\n            table = \"ways_line\"\n        if geometry.geom_type == \"Polygon\":\n            table = \"ways_poly\"\n        if geometry.geom_type == \"Point\":\n            table = \"nodes\"\n        tags = f\"\\'{{\"\n        for key, value in entry[\"properties\"].items():\n            tags += f\"\\\"{key}\\\": \\\"{value}\\\", \"\n        tags = tags[:-2]\n        tags += \"}\\'::jsonb)\"\n        sql = f\"INSERT INTO {table} (geom, tags) VALUES(ST_GeomFromEWKT(\\'SRID=4326;{ewkt}\\'), {tags}\"\n        result = await self.db.pg.execute(sql)\n\n    return False\n</code></pre>"},{"location":"api/#osm_merge.geosupport.GeoSupport.initialize","title":"initialize  <code>async</code>","text":"<pre><code>initialize(dburi=None, config=None)\n</code></pre> <p>When async, we can't initialize the async database connection, so it has to be done as an extrat step.</p> <p>Parameters:</p> Name Type Description Default <code>dburi</code> <code>str</code> <p>The database URI</p> <code>None</code> <code>config</code> <code>str</code> <p>The config file from the osm-rawdata project</p> <code>None</code> Source code in <code>osm_merge/geosupport.py</code> <pre><code>async def initialize(self,\n                    dburi: str = None,\n                    config: str = None,\n                    ):\n    \"\"\"\n    When async, we can't initialize the async database connection,\n    so it has to be done as an extrat step.\n\n    Args:\n        dburi (str, optional): The database URI\n        config (str, optional): The config file from the osm-rawdata project\n    \"\"\"\n    if dburi:\n        self.db = PostgresClient()\n        await self.db.connect(dburi)\n    elif self.dburi:\n        self.db = PostgresClient()\n        await self.db.connect(self.dburi)\n\n    if config:\n        await self.db.loadConfig(config)\n    elif self.config:\n        await self.db.loadConfig(config)\n</code></pre>"},{"location":"api/#osm_merge.geosupport.GeoSupport.clipDB","title":"clipDB  <code>async</code>","text":"<pre><code>clipDB(boundary, db=None, view='ways_view')\n</code></pre> <p>Clip a database table by a boundary</p> <p>Parameters:</p> Name Type Description Default <code>boundary</code> <code>Polygon</code> <p>The AOI of the project</p> required <code>db</code> <code>PostgresClient</code> <p>A reference to the existing database connection</p> <code>None</code> <code>view</code> <code>str</code> <p>The name of the new view</p> <code>'ways_view'</code> <p>Returns:</p> Type Description <code>bool</code> <p>If the region was clipped sucessfully</p> Source code in <code>osm_merge/geosupport.py</code> <pre><code>async def clipDB(self,\n         boundary: Polygon,\n         db: PostgresClient = None,\n         view: str = \"ways_view\",\n         ):\n    \"\"\"\n    Clip a database table by a boundary\n\n    Args:\n        boundary (Polygon): The AOI of the project\n        db (PostgresClient): A reference to the existing database connection\n        view (str): The name of the new view\n\n    Returns:\n        (bool): If the region was clipped sucessfully\n    \"\"\"\n    remove = list()\n    if not boundary:\n        return False\n\n    ewkt = shape(boundary)\n\n    # Create a new postgres view\n    # FIXME: this should be a temp view in the future, this is to make\n    # debugging easier.\n    sql = f\"DROP VIEW IF EXISTS {view} CASCADE ;CREATE VIEW {view} AS SELECT * FROM ways_poly WHERE ST_CONTAINS(ST_GeomFromEWKT('SRID=4326;{ewkt}'), geom)\"\n    # log.debug(sql)\n    if db:\n        result = await db.queryDB(sql)\n    elif self.db:\n        result = await self.db.queryDBl(sql)\n    else:\n        return False\n\n    return True\n</code></pre>"},{"location":"api/#osm_merge.geosupport.GeoSupport.queryDB","title":"queryDB  <code>async</code>","text":"<pre><code>queryDB(sql=None, db=None)\n</code></pre> <p>Query a database table</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>PostgreClient</code> <p>A reference to the existing database connection</p> <code>None</code> <code>sql</code> <code>str</code> <p>The SQL query to execute</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>The results of the query</p> Source code in <code>osm_merge/geosupport.py</code> <pre><code>async def queryDB(self,\n            sql: str = None,\n            db: PostgresClient = None,\n            ) -&gt; list:\n    \"\"\"\n    Query a database table\n\n    Args:\n        db (PostgreClient, optional): A reference to the existing database connection\n        sql (str): The SQL query to execute\n\n    Returns:\n        (list): The results of the query\n    \"\"\"\n    result = list()\n    if not sql:\n        log.error(f\"You need to pass a valid SQL string!\")\n        return result\n\n    if db:\n        result = db.queryLocal(sql)\n    elif self.db:\n        result = self.db.queryLocal(sql)\n\n    return result\n</code></pre>"},{"location":"api/#osm_merge.geosupport.GeoSupport.clipFile","title":"clipFile  <code>async</code>","text":"<pre><code>clipFile(boundary, data)\n</code></pre> <p>Clip a database table by a boundary</p> <p>Parameters:</p> Name Type Description Default <code>boundary</code> <code>Polygon</code> <p>The filespec of the project AOI</p> required <code>data</code> <code>FeatureCollection</code> <p>The data to clip</p> required <p>Returns:</p> Type Description <code>FeatureCollection</code> <p>The data within the boundary</p> Source code in <code>osm_merge/geosupport.py</code> <pre><code>async def clipFile(self,\n            boundary: Polygon,\n            data: FeatureCollection,\n            ):\n    \"\"\"\n    Clip a database table by a boundary\n\n    Args:\n        boundary (Polygon): The filespec of the project AOI\n        data (FeatureCollection): The data to clip\n\n    Returns:\n        (FeatureCollection): The data within the boundary\n    \"\"\"\n    new = list()\n    if len(self.data) &gt; 0:\n        for feature in self.data[\"features\"]:\n            shapely.from_geojson(feature)\n            if not shapely.contains(ewkt, entry):\n                log.debug(f\"CONTAINS {entry}\")\n                new.append(feature)\n                #  del self.data[self.data['features']]\n\n    return new\n</code></pre>"},{"location":"api/#osm_merge.geosupport.GeoSupport.copyTable","title":"copyTable  <code>async</code>","text":"<pre><code>copyTable(table, remote)\n</code></pre> <p>Use DBLINK to copy a table from the external database to a local table so conflating is much faster.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>The table to copy</p> required Source code in <code>osm_merge/geosupport.py</code> <pre><code>async def copyTable(self,\n                    table: str,\n                    remote: PostgresClient,\n                    ):\n    \"\"\"\n    Use DBLINK to copy a table from the external\n    database to a local table so conflating is much faster.\n\n    Args:\n        table (str): The table to copy\n    \"\"\"\n    timer = Timer(initial_text=f\"Copying {table}...\",\n                  text=\"copying {table} took {seconds:.0f}s\",\n                  logger=log.debug,\n                )\n    # Get the columns from the remote database table\n    self.columns = await remote.getColumns(table)\n\n    print(f\"SELF: {self.pg.dburi}\")\n    print(f\"REMOTE: {remote.dburi}\")\n\n    # Do we already have a local copy ?\n    sql = f\"SELECT FROM pg_tables WHERE schemaname = 'public' AND tablename  = '{table}'\"\n    result = await self.pg.execute(sql)\n    print(result)\n\n    # cleanup old temporary tables in the current database\n    # drop = [\"DROP TABLE IF EXISTS users_bak\",\n    #         \"DROP TABLE IF EXISTS user_interests\",\n    #         \"DROP TABLE IF EXISTS foo\"]\n    # result = await pg.pg.executemany(drop)\n    sql = f\"DROP TABLE IF EXISTS new_{table} CASCADE\"\n    result = await self.pg.execute(sql)\n    sql = f\"DROP TABLE IF EXISTS {table}_bak CASCADE\"\n    result = await self.pg.execute(sql)\n    timer.start()\n    dbuser = self.pg.dburi[\"dbuser\"]\n    dbpass = self.pg.dburi[\"dbpass\"]\n    sql = f\"CREATE SERVER IF NOT EXISTS pg_rep_db FOREIGN DATA WRAPPER dblink_fdw  OPTIONS (dbname 'tm4');\"\n    data = await self.pg.execute(sql)\n\n    sql = f\"CREATE USER MAPPING IF NOT EXISTS FOR {dbuser} SERVER pg_rep_db OPTIONS ( user '{dbuser}', password '{dbpass}');\"\n    result = await self.pg.execute(sql)\n\n    # Copy table from remote database so JOIN is faster when it's in the\n    # same database\n    #columns = await sel.getColumns(table)\n    log.warning(f\"Copying a remote table is slow, but faster than remote access......\")\n    sql = f\"SELECT * INTO {table} FROM dblink('pg_rep_db','SELECT * FROM {table}') AS {table}({self.columns})\"\n    print(sql)\n    result = await self.pg.execute(sql)\n\n    return True\n</code></pre>"},{"location":"boundaries/","title":"Boundaries","text":"<p>Good boundaries are critical to being able to chop the large files into manageble pieces.</p>"},{"location":"boundaries/#boundary-sources","title":"Boundary Sources","text":""},{"location":"boundaries/#openstreetmap","title":"OpenStreetMap","text":"<p>It is possible to pull boundaries out of OpenStreetMap, but a warning, many aren't very good. In OSM, some boundaries are a Way (Polygon) some are relations, some are missing sections. Extracting many boundaries is not for the faint of heart... There is a website attempting to do this. If you only need a few boundaries, OSM works fine, but you might have to do a little manual cleanup.</p>"},{"location":"boundaries/#administrative-sources","title":"Administrative Sources","text":"<p>I use official administrative boundary datasets. While these are all public domain, I have no interest in uploadinmg them to OSM, which would be a whole other project. And I only need them for data chopping anyway. You can get the official boundaries from these two sources.</p> <p>National Forests</p> <p>National Parks</p> <p>These datasets are national wise, so need to be split into each forest or park. I wrote a program called  tm-splitter that reads in the large file, and then splits each forest and park into a standalone file. This standlone file is a MultiPolygon, as many forests have multiple sections that aren't actually connected. But at this point, you have small enough boundaries to start on making data extracts for conflation.</p>"},{"location":"boundaries/#boundary-problems","title":"Boundary Problems","text":"<p>Most of the National Park boundaries are ofte a single polygon, as many national parks are smaller than national forests. National Forest boundaries have an interesting set of problems that need to be cleaned up before they're usable.</p>"},{"location":"boundaries/#small-outholdings","title":"Small Outholdings","text":"<p>National Forests often have multiple small Polygons that are outside of the actual forest boundary. These appear to be administrative buildings. Ranger stations, visitors centers, etc... These are all useles for our goal of improving remote trails and highways.</p> <p>When the tm-splitter utility parses each MultiPolygon into it's indivugal Polygons, these small outholdings are ignored, leaving only the actual forest boundaries.</p>"},{"location":"boundaries/#inner-polygons","title":"Inner Polygons","text":"<p>Since we're only interested in using these boundaries for making data extracts, some have interior Polygons that have other ownership or public land designations. Since we're using boundaries to make data extracts, I may drop all inner Polygons, but so far they don't see to cause any problems.</p>"},{"location":"calculations/","title":"Conflation Calculations","text":"<p>Part of the fun of external datasets, especially some that have been around long time like the MVUM data is the the variety of inconsistencies in the data. While OpenStreetMap itself is a bit overly flexible at time, so is external data. And some of the old data has been converted from other formats several times, with bugs getting introduced each time.</p>"},{"location":"calculations/#geometries","title":"Geometries","text":"<p>OpenStreetMap has relations, which are a collection of references to other features. External data may have LineStrings, MultiLineStrings or a GeometryCollection, all in the same file! For all calculations the MultiLineString and GeometryCollections are taken apart, so the calculations are between OSM data and that segment of the external data. Since this may product multiple values, those need to be evaluated and the most likely one returned.</p> <p>It gets more fun as sometimes the MVUM dataset is missing entire segments. Course sometimes OSM is too. conflation sucessfully merges the MVUM dataset tags for the segments if they match onto the single OSM way.</p> <p>![Screenshot\\ from\\ 2024-10-19\\ 14-06-00.png])</p>"},{"location":"calculations/#distance","title":"Distance","text":"<p>A simple distance calculation is performed after transforming the coordinate system from global degrees to meters. The result is compared to a threshold distance, and any feature within that threshold is added to a list of possible matches. After a few features are found in the required distance, matching stops and then the next feature to be conflated is started on the same process.</p> <p>If the highway is a GeometryCollection or MultiLineString, then it's split into segments, and each one is checked for distance. The closest one is what is returned.</p>"},{"location":"calculations/#slope-and-angle","title":"Slope and Angle","text":"<p>Distance often will return features that are close to each other, but often they are spur roads off the more major one. So when two highway segments are found close to each other, the angle between them is calculated. This works well to differentiate between the more major highway, and the spur road that splits off from that.</p> <p>If the highway is a GeometryCollection or MultiLineString, then it's split into segments, and each one is checked for the angle. The closest one is what is returned.</p> <p>Sometimes the geometry of the feaure in OSM was imported from the same external dataset. At that point it's an exact match, so the distance, the slope, and the angle will all be 0.0.</p>"},{"location":"calculations/#tag-checking","title":"Tag Checking","text":"<p>Once there is at least one candidate within the parameters of distance and angle, then the tags are checked for matches. The tags we are primarily interested in are name(s) and reference number(s) of each MVUM road or trail. Some of the existing features in OpenStreetMap may be inaccurate as to the proper name and reference. And of course each feature may have an alt_name or both a ref and a ref:usfs. Due to the wonders of inconsistent data, a fuzzy string comparison is done. This handles most of the basic issues, like capitalization, one or 2 characters difference, etc... Anything above the threshold is considered a probably match, and increments a counter. This value is included in the conflated results, and is often between 1-3.</p> <p>The reference numbers between the two datasets is also compared. There is often a reference number in OSM already, but no name. The external dataset has the name, so we want to update OSM with that. In addition, the external datasets often have access information. Seasonal access, private land, or different types of vehicles which can be added to OSM.</p>"},{"location":"calculations/#tag-merging","title":"Tag Merging","text":"<p>The conflation process for merging tags uses the concept of primary and secondary datasets. The primary is considered to have the true value for a highway or trail. For example, if the name in the two datasets doesn't match, the secondary will then rename the current value to old_something. The primary's version becomes the same. Some with reference numbers.</p> <p>Other tags from the primary can also be merged, overriding what is currently in OSM. Once again, the old values are renamed, not deleted. When validating in JOSM, you can see both versions and make a final determination as to what is the correct value. Often it's just spelling differences.</p> <p>For all the features in OSM that only have a highway=something as a tag, all the desired tags from the primary dataset are added.</p> <p>For some tags like surface and smoothness, the value in OSM is potentially more recent, so those are not updated. For any highway feature lacking those tags, they get added.</p> <p>Optionally the various access tags for private, atv, horse, motorcycle, etc... are set in the post conflation dataset if they have a value in the external dataset. </p>"},{"location":"calculations/#debug-tags","title":"Debug Tags","text":"<p>Currently a few tags are added to each feature to aid in validating and debugging the conflation process. These should obviously be removed before uploading to OSM. They'll be removed at a future date after more validation. These are:</p> <ul> <li>hits - The number of matching tags in a feature</li> <li>ratio - The ratio for name matching if not 100%</li> <li>dist -  The distance between features</li> <li>angle - The angle between two features</li> <li>slope - The slope between two features</li> </ul>"},{"location":"calculations/#issues","title":"Issues","text":"<p>Conflation is never 100% accurate due to the wonderful um... \"flexibility\" of the datasets. Minor tweaks to the steering parameters for the distance, angle, and fuzzy string matching can produce slightly different results. I often run the same datasets with different parameters looking for the best results.</p>"},{"location":"calculations/#clipping","title":"Clipping","text":"<p>Where a feature crosses the task boundary, the calculations have to deal with incomplete features, which is messy. This is particularly a problem when conflating small datasets.</p>"},{"location":"conflation/","title":"Conflating External Datasets","text":"<p>This project is the merging of several programs for conflating external datasets with OpenStreetMap data developed at HOT. These were originally developed for large scale building imports using MS Footprints in East Africa, and to also work with conflating data collected with OpenDataKit for the Field Mapping Tasking Manager project.</p>"},{"location":"conflation/#the-data-files","title":"The Data Files","text":"<p>While any name can be used for the OSM database, I usually default to naming the OpenStreetMap database the country name as used in the data file. Other datasets have their own schema, and can be imported with ogr2ogr, or using python to write a custom importer. In that case I name the database after the dataset source. Past versions of this program could conflate between multiple datasets, so it's good to keep things clear.</p>"},{"location":"conflation/#overture-data","title":"Overture Data","text":"<p>The Overture Foundation (https://www.overturemaps.org) has been recently formed to build a competitor to Google Maps. The plan is to use OpenStreetMap (OSM) data as a baselayer, and layer other datasets on top. The currently available data (July 2023) has 13 different datasets in addition to the OSM data. It is available here. It also includes a snapshot  of OSM data from the same time frame. Other than the OSM data and MS Footprints, all the current additional data is primarily US specific, and often contains multiple copies of the same dataset, but from different organization.</p> <p>The osm-rawdata python module has a utility that'll import the Parquet data files into the postgress database schema used by multiple projects at HOT. That schema is designed for data analysis, unlike the standard OSM database schema. There is more detail in these notes I've written about importing Overture Data into postgres.</p>"},{"location":"conflation/#duplicate-buildings","title":"Duplicate Buildings","text":"<p>This is the primary conflation task. Because of offsets in the satellite imagery used for the original buildings, there is rarely an exact duplicate, only similar. The only times when you see an exact duplicate, it's because the same source data is in multiple other datasets. The orientation may be different even if the same rough size, or it'll be roughly in the same position, but differing sizes. Several checks are made to determine duplicates. First is to check for any intersection of the two polygons. If the two polygons intersection it's an overlapping building or possibly duplicate. Any building in the footprint data that is found to be a duplicate is removed from the output data file.</p>"},{"location":"conflation/#overlapping-buildings","title":"Overlapping Buildings","text":"<p>It is entirely possible that a new building in the footprints data may overlap with an existing building in OSM. It wouldn't be overlapping in the footprints data. Since this requires human intervention to fix, these buildings are left in the output data, but flagged with a debugging tag of overlapping=yes. There is also many occurances where the building being imported has a better building geometry than OSM, so the best one should be selected.</p> <p>Using the HOT Underpass project, it is possible to scan the building geometries and either delete the bad geometry one, or flag it in the result data files for a human to validate the results.</p>"},{"location":"conflation/#known-problems","title":"Known Problems","text":"<p>There are two main issues with ML/AI derived building footprints, Buildings that are very close together, like the business section in many areas of the world, do not get marked as separate buildings. Instead the entire block of buildings is a single polygon. This will eventually get fixed by drone mapping, where there can be more of a street view of the buildings that you can't get using existing satellite imagery.</p> <p>The other problem is that as processing satellite imagery is that buildings are recognized by shading differences, so often features are flagged as buildings that don't actually exist. For example, big rocks in the desert, or haystacks in a field both get marked as a building. Any building in the footprints data that has no other buildings nearby, nor a highway or path of some kind, is flagged with a debugging tag of false=yes. Usually this is easy to determine looking at satellite imagery, since these are often remote buildings. The tags can be searched for when editing the data to visually determine whether it's a real building or not.</p>"},{"location":"conflation/#conflating-other-than-buildings","title":"Conflating Other Than Buildings","text":""},{"location":"conflation/#opendatakit","title":"OpenDataKit","text":"<p>Data collected in the field using ODK Collect is a specific case. If using using data extracts from OpenStreetMap, the data extract has the OSM ID, so it's much simpler to conflate the new tags with either the existing building polygon or POI. For this workflow, any tag in the feature from ODK will overwrite any existing values in the existing feature. This allows for updating the tags &amp; values when ground-truthing. When the OSM XML file is loaded into JOSM, it has the modified attribute set, and the version has been incremented. In JOSM under the File menu, select the Update Modified menu item. This will sync the modified feature with current OSM. At that point all that needs to be done is validate the modified features, and upload to OSM.</p> <p>When ODK Collect is used but has no data extract, conflation is more complicated. For this use case, a more brute force algorythm is used. Initially any building polygon or POI within 7 meters is found by querying the database. Most smartphone GPS chipsets, even on high-end phones, are between 4-9m off from your actual location. That value was derived by looking at lots of data, and can be changed when invoking the conflation software in this project. Once nearby buildings are identified, then the tags are compared to see if there is a match.</p> <p>For example, if collecting data on a restaurant, it may have a new name, but if the nearby building is the only one with an amenity=restaurant** (or cafe, pub, etc...) it's considered a probable match. If there are multiple restaurants this doesn't work very well unless the name hasn't changed. If there are multiple possible features, a *fixme= tag is added to the POI, and it has to be later validated manually. Every tag in the ODK data has to be compares with the nearby buildings. Often it's the name tag that is used for many amenities.</p> <p>If a satellite imagery basemap is used in Collect, conflation is somewhat simpler. If the mapper has selected the center of the building using the basemap, conflation starts by checking for the building polygon in OSM that contains this location. If no building is found, the POI is added to the output file with a fixme=new building tag so the buildings can traced by the validator. Any tags from the POI are added to the new building polygon.</p>"},{"location":"conflation/#points-of-interest-poi","title":"Points Of Interest (POI)","text":"<p>It is common when collecting datasets from non-OSM sources each feature may only be single node. This may be a list of schools, businesses, etc... with additional information with each POI that can be added to the OSM building polygon (if it exists). Obviously any imported data must have a license acceptable for importing into OSM.</p> <p>Similar to how conflating ODK data when not using a data extract, the tags &amp; values are compared with any nearby building. Since often these imports are features already in OSM with limited metadata, this adds more details.</p>"},{"location":"conflation/#highways","title":"Highways","text":"<p>Highways are more complex because it uses relations. A relation is a groups of highway segments into a single entity. Some times the tags are on the relation, other times each highway segment. The segments change when the highway condition changes, but the name and reference number doesn't change. External datasets don't use relations, they are OSM specific.</p>"},{"location":"conflation/#mvum-highways","title":"MVUM Highways","text":"<p>The USDA publishes a dataset of Motor Vehicle Use Maps (MVUM) highways in the National Forest. Some of this data has already been imported into OSM, although the metadata may be lacking, but the LineString is there. MVUM roads are primarily compacted dirt roads. While some can be driven in a passenger vehicle, most are varying degrees of bad to horrible to impassable. These highways are often used for recreational traffic by off-road vehicles, or for emergency access for a wildland fire or backcountry rescue.</p> <p>Another key detail of MVUM highways is each one may have 4 names! There is of course the primary name, for example \"Cedar Lake Road\". But it may also have a locals name, common in remote areas. And then there is the reference number. A MVUM highway may have two reference numbers, the country designated one, and the USDA one. Luckily OSM supports this. Many of these tags effect both how the highway is displayed, as well as routing for navigation. </p> <pre><code>\"name\": \"Platte Lake Road\",\n\"alt_name\": \"Bar-K Ranch Road\",\n\"surface\": \"dirt\",\n\"smoothness\": \"bad\",\n\"highway\": \"track\",\n\"ref\": \"CO 112\",\n\"ref:usfs\": \"FR 521.1A\"\n\"tracktype\": \"grade3\"\n</code></pre> <p>A bad highway is something I'd be comfortable driving in a 4x4 high-clearance vehicle. Smoothness values can be a bit misleading, as often what is in OSM may be years out of date. And most MVUM roads get zero maintainance, so get eroded, pot-holed, and or exposed rocks. And people's perception of road conditions is subjective based on one's experience driving these highways.</p> <p>All of this metadata makes conflation interesting. Since existing OSM features were added by more than one person, the tagging may not be consistent. For example, the existing data may have Forest Service Road 123, which should really be ref:usfs=FR 123. And the real highway name Piney Pass Road is in the MVUM dataset. The goal of highway conflation is to merge the new metadata into the existing OSM feature where possible. This then needs to be validated by a human being. There is still much tedious work to process post conflation data before it can be uploaded to OSM.</p> <p>But sometimes conflation works well, especially when the LineString in OSM was imported from older versions of the MVUM data. But often highways in OSM were traced off satellite imagery, and may have wildly different geometry.</p> <p>If you ignore conflating the tags other than name or ref, the process is somewhat less messy. And tags like surface and smoothness really should be ground-truthed anyway. So I do ignore those for now and stick to validating the name and the two reference numbers which are usually lacking in OSM. That and addding consistency to the data to make it easier to make data extracts.</p> <p>To conflate OSM highways with external data, initially each entry in the external dataset does a distance comparison with the existing OSM data. There is an optional threshold to set the distance limit. Since currently this is focused on conflating files without a database, this is computationally intensive, so slow. For data that was imported in the past from MVUM datasets, a distance of zero means it's probably the same segment. The external dataset needs to have the tagging converted to the syntax OSM uses. Tagging can be adjusted using a conversion program, but as conversion is usually a one-off task, it can also be done using JOSM or QGIS. Usually it's deleting most of the tags in the external dataset that aren't appropriate for OSM. Primarily the only tags that are needed are the name and any reference numbers. Since the MVUM data also classified the types of road surface, this can also be converted. Although as mentioned, may be drastically out of data, and OSM is more recent and ground-truthed.</p> <p>Then there is a comparison of the road names. It's assumed the one from the MVUM dataset is the correct one. And since typos and weird abbreviations may exist in the datasets, fuzzy string matching is performed. This way names like FS 123.1 can match FR 123.1A. In this case the current name value in OSM becomes alt_name, and the MVUM name becomes the official name. This way when validating you can make decisions where there is confusion on what is correct. For an exact name match no other tags are checked to save a little time.</p> <p>Any other processing is going to be MVUM highway specific, so there will be an additional step to work through the reference numbers not supported by this program.</p>"},{"location":"conflation/#output-files","title":"Output Files","text":"<p>If the data files are huge, it's necessary to conflate with a subset of all the data. For projects using the Tasking Manager or the Field Mapping Tasking Manager you can download the project boundary file and use that. For other projects you can extract administrative bondaries from OpenStreetMap, or use external sources. Usually county administrative boundaries are a good size. These can be extracted from OSM itself, or an external data file of boundaries.</p> <p>After conflation, an output file is created with the new buildings that are not duplicates of existing OSM data. This is much smaller than the original data, but still too large for anyone having bandwidth issues. This output file is in GeoJson format, so can be edited with JOSM or QGIS</p> <p>Since this software is under development, rather than automatically deleting features, it adds tags to the features. Then when editing the data, it's possible to see the flagged data and validate the conflation. It also makes it possible to delete manually the results of the conflation from the output file once satisfied about the validation of the results.</p>"},{"location":"conflation/#validating-the-conflation","title":"Validating The Conflation","text":"<p>The conflated data file can't be uploaded to OSM until it is validated. While QGIS can be used for this purpose, JOSM is preferred because it does validation checks, and uploads directly to OpenStreetMap. I start by loading the conflation data file, and then enabling the OpenStreetMap imagery for the basemap. Existing buildings in OSM are grey polygons, so it's possible to see existing buildings with the conflated new buildings as a layer on top.</p> <p>Once the buildings are loaded, you can then download the OSM data for that view. Then use the SelectDuplicateBuilding script to find any buildings that have been added since the initial data file for conflation was used. Once selected, those can be deleted in a single operation.</p> <p>The next step is validating what is left that is considered to be a new building. This is done using satellite imagery. Most commercial satellite imagery available for public use comes from Maxar. But the different providers (Bing, ESRI, Google, etc...) have different update cycles, so I often double check with ESRI imagery.</p> <p>If there is drone imagery available from Open Aerial Map, that's also a good surce of imagery, but often doesn't cover a large area.</p>"},{"location":"dataflow/","title":"Mapper Data flow","text":"<p>Much of the process of conflation is preparing the datasets since we're dealing with huge files with inconsistent metadata. The primary goal is to process the data so validating the post conflation is as efficient as possible. Conflating large datasets can be very time consuming, so working with smaller files generates results quicker for the area you are focused on mapping.</p> <p>The other goal is to prepare the data for Tasking Manager (TM). TM has a project size limit of 5000km sq, and since we'll be using the Tasking Manager, each national forest or park needs to be split into project sized areas of interest. Each of these is used when creating the TM project.</p> <p>When you select a task in the TM project, it'll download an OSM extract and satellite imagery for that task. We don't really need those, as we're dealing with disk files, not remote mapping. While it's entirely possible to use the project sized data extracts, I also create a custom task boundaries files for TM, and make small task sized extracts that are relatively quick to conflate and validate.</p>"},{"location":"dataflow/#download-the-datasets","title":"Download the Datasets","text":"<p>All the datasets are of course publicly available. The primary source of the Motor Vehicle Use Map (MVUM) is available from the  FSGeodata Clearinghouse, which is maintained by the USDA. The Topographical map vector tiles are available from here., which is maintained by the National Forest Service. OpenStreetMap data for a country can be downloaded from Geofabrik. National Park trail data is available from the NPS Publish site.</p>"},{"location":"dataflow/#initial-setup","title":"Initial Setup","text":"<p>As we split up the initial datasets this will generate a lot of files if you plan to work with multiple national forests or parks. I use a tree structure. At the top is the directory with all the source files. You also need a directory with the national forest or park boundaries which get used for data clipping.</p> <p>Once I have the source files ready, I start the splitting up process to make data extracts for each forest or park. If you are only working on one forest or park, you can do this manually. Since I'm working with data for multiple states, I wrote a shell script to automate the process.</p>"},{"location":"dataflow/#updatesh","title":"update.sh","text":"<p>Most of the process is executing other external programs like osmium or ogr2ogr, so I wrote a bourne shell script to handle all the repetitious tasks. This also lets me easily regenerate all the files if I make a change to any of the utilities or the process. This uses a modern shell syntax with functions and data structures to reduce cut &amp; paste.</p> <p>The command line options this program supports are:</p> <pre><code>--tasks (-t): Split tasks boundaries into files for ogr2ogr\n--forests (-f): Build only the National Forests\n--datasets (-d): Build only this dataset for all boundaries\n--split (-s): Split the AOI into tasks, also very slow\n--extract (-e): Make a data extract from OSM\n--only (-o): Only process one state\n--dryrun (-n): Don't actually write any datafiles\n--clean (-c): Remove generated task files\n--base (-b): build all base datasets, which is slow\n</code></pre> <p>The locations of the files is configurable, so it can easily be extended for other forests or parks. This script is in the utilities directory of this project.</p> <p>This also assumes you want to build a tree of output directories.</p> <p>For example I use this layout:</p> <pre><code>SourceData\n    -&gt; Tasks\n        -&gt; Colorado\n            -&gt; Medicine_Bow_Routt_National_Forest_Tasks\n                -&gt; Medicine_Bow_Routt_Task_[task number]\n            -&gt; Rocky_Mountain_National_Park_Task\n                -&gt; Rocky_Mountain_National_Park_Task_[task number]\n        -&gt; Utah\n            -&gt; Bryce_Canyon_National_Park_Tasks\n        etc...\n</code></pre> <p>All my source datasets are in SourceData.   In the Tasks directory I have all the Multi Polygon files for each forest or park. I create these files by running update.sh --split. These are the large files that have the AOI split into 5000-km sq polygons.</p> <p>Since I'm working with multiple states, that's the next level, and only contains the sub directories for all the forests or parks in that state. Currently I have all the data for all the public lands in Colorado, Utah, and Wyoming. Under each sub directory are the individual task polygons for that area. If small TM task sized data extracts are desired, all of the small tasks is under the last directory. Those task files are roughly 10km sq.</p>"},{"location":"dataflow/#boundaries","title":"Boundaries","text":"<p>You need boundaries with a good geometry. These can be extracted from OpenStreetMap, they're usually relations. The official boundaries are also available from the same site as the datasets as a Multi Polygon.</p> <p>I use the TM Splitter utility included in this project to split the Multi Polygon into separate files, one for each forest or park. Each of these files are also a Multi Polygon, often a national forest has several areas that aren't connected.</p>"},{"location":"dataflow/#processing-the-data","title":"Processing The Data","text":"<p>To support conflation, all the datasets need to be filtered to fix known issues, and to standardize the data. The OpenStreetMap tagging schema is used for all data.</p> <p>Each of the external datasets has it's own conversion process, which is documented in detail here:</p> <ul> <li>MVUM</li> <li>Trails</li> <li>OSM</li> </ul> <p>While it's possible to manually convert the tags using an editor, it can be time consuming. There are also many, many weird and inconsistent abbreviations in all the datasets. I extracted all the weird abbreviations by scanning the data for the western United States, and embedding them in the conversion utilities. There are also many fields in the external datasets that aren't for OSM, so they get dropped. The result are files with only the tags and features we want to conflate. These are the files I put in my top level SourceData directory.</p>"},{"location":"dataflow/#conflation","title":"Conflation","text":"<p>Once all the files and infrastructure is ready, then I can conflate the external datasets with OpenStreetMap. Here is a detailed description of conflating highways. Conflating with  OpenDataKit is also documented. The final result of conflation is an OSM XML file for JOSM. The size of this file is determined by task boundaries you've created.</p> <p>If you want to use TM, then create the project with the 5000km sq task boundary, and fill in all the information required. Then select your task from the TM project and get started with validation.</p>"},{"location":"dataflow/#validation","title":"Validation","text":"<p>Now the real fun starts after all this prep work. The goal is to make this part of the process, validating the data and improving OSM as efficient as possible. If it's not efficient, manual conflation is incredibly time-consuming, tedious, and boring. Which is probably why nobody has managed to fix more than a small area.</p> <p>The conflation results have all the tags from the external datasets that aren't in the OSM feature or have different values. Any existing junk tags have already been deleted. The existing OSM tags are renamed where they don't match the external dataset, so part of validation is choosing the existing value or the external one, and delete the one you don't want. Often this is a minor difference in spelling.</p> <p>If the conflation has been good, you don't have to edit any features, only delete the tags you don't want. This makes validating a feature quick, often in under a minute per feature. Since many remote MVUM roads are only tagged in OSM with highway=track, validating those is very easy as it's just additional tags for surface, smoothness, and various access tags.</p> <p>In the layer in JOSM with the conflated data, I can select all the modified features, and load them into the TODO plugin. Then I just go through them all one at a time to validate the conflation. I also have the original datasets loaded as layers, and also use the USGS Topographical basemaps in JOSM for those features I do need to manually edit. Even good conflation is not 100%.</p>"},{"location":"formats/","title":"File Formats","text":"<p>This project support two file formats, GeoJson and OSM XML. </p>"},{"location":"formats/#geojson","title":"GeoJson","text":"<p>GeoJson is widely supported by many tools, and this project uses it as the internal data structure for consistency. At the top level the file starts with a GeometryCollection, which is just a container for the list of features.</p>"},{"location":"formats/#geometry","title":"Geometry","text":"<p>Each GeoJson feature contains a geometry object that has two fields, the coordinates, and the type. Shapely or GDAL can be used to convert between string representations and geometry objects.</p>"},{"location":"formats/#properties","title":"Properties","text":"<p>The properties is the array of keyword=value pairs, similar to the tags in OSM. There is no definition of a schema, and pair works. For conflation though, standardizing on the OSM schema for tagging pairs is critical to keep things simple.</p> <pre><code>\"properties\": {\n    \"ref:usfs\": \"FR 965.2\",\n    \"name\": \"  Road\",\n    \"4wd_only\": \"yes\",\n    \"seasonal\": \"yes\"\n},\n</code></pre>"},{"location":"formats/#osm-xml","title":"OSM XML","text":"<p>An OSM XML file is read and converted to GeoJson, and then later it can get converted to OSM XML for the output file. In addition to the tags and geometry, each feature also has attributes.</p>"},{"location":"formats/#attributes","title":"Attributes","text":"<p>The OSM XML format has attributes, which are used to control editing a feature. Since this project wants to generate an OSM XML file for JOSM that allows for tag merging, these attributes are important. In the post conflation data file, the version of the existing OSM feature has been incremented, and the action is set to modify. This enable JOSM to see this as an edited feature so it can be uploaded.</p> <ul> <li>id - the OSM ID of the feature</li> <li>version - the current version of the feature</li> <li>action - the action to apply when uploading to OSM<ul> <li>create</li> <li>modify</li> <li>delete</li> </ul> </li> <li>timestamp - the timestamp of the feature's last change</li> </ul> <p>With action=modify set, in JOSM you can update modified and sync with current OSM.</p>"},{"location":"formats/#data-types","title":"Data Types","text":"<p>There are two data types in the OSM XML files used for conflation. These are nodes and ways.</p>"},{"location":"formats/#nodes","title":"Nodes","text":"<p>A node is a single coordinate. This is often used as a POI, and will have tags. A node that is referenced in a way won't have any tags, just the coordinates. The version and timestamp get updated if there is a change to the node location.</p> <pre><code>&lt;node id=\"83276871\" version=\"3\"\n    timestamp=\"2021-06-12T16:25:43Z\" lat=\"37.6064731\" lon=\"-114.00674\"/&gt;\n</code></pre>"},{"location":"formats/#ways","title":"Ways","text":"<p>A way can be a linestring, polygon, any geometry that includes more than one node. This makes it difficult to do spatial comparisons, so when an OSM XML file is loaded, in addition to the refs, they are also converted to an actual geometry. All the calculations use the geometry, and the refs are used to construct the OSM XML output file for JOSM. OSM has no concept of a LineString or Polygon, the shape is determined by the tags, for example highway=track, or building=yes.</p> <pre><code>&lt;way id=\"10109556\" version=\"4\" timestamp=\"2021-06-12T15:42:25Z\"&gt;\n&lt;nd ref=\"83305252\"/&gt;\n&lt;nd ref=\"8118009676\"/&gt;\n&lt;nd ref=\"8118009677\"/&gt;\n&lt;nd ref=\"83277113\"/&gt;\n&lt;nd ref=\"83277114\"/&gt;\n&lt;nd ref=\"83277116\"/&gt;\n&lt;nd ref=\"83277117\"/&gt;\n&lt;tag k=\"highway\" v=\"unclassified\"/&gt;\n&lt;tag k=\"surface\" v=\"dirt\"/&gt;\n</code></pre> <p></p>"},{"location":"formats/#converting-between-formats","title":"Converting Between Formats","text":"<p>To support reading and writing OSM XML files, this project has it's own code that builds on top of the OsmFile() class in the OSM Fieldwork. This parses the OSM XML file into GeoJson format for internal use. All of the attributes in the OSM XML file being read are convert to tags in the GeoJson properties section, and then later converted from the properties back to OSM XML attributes when writing the output file.</p>"},{"location":"highways/","title":"Conflating Highway and Trail Data","text":"<p>This is focused only on highway and trail data in the US, but should be useful for other countries. In particular, this is focused on the primary goal of improving OpenStreetMap data in remote areas as these are used for emergency response. Most of these roads and trails are in OSM already, some from past imports, some traced off of satellite imagery. </p> <p>I did a talk at SOTM-US in Tucson about this project called OSM For Fire Fighting. This conflation software was developed to improve the quality of the remote highway data in OpenStreetMap. This is not an import of new data, only updating existing features with a focus on improved navigation. Importing new features from these datasets uses a different process, so it's better to not mix the two.</p> <p></p> <p>While there are details in the the datasets that would be useful, the initial set is the name, the reference number, and the vehicle class appropriate for this highway. Not this can change over time, so if the smoothness tag is in the OSM feature, it's assumed that value is more accurate.</p> <p>The primary purpose is to clean up the TIGER import mess, which is often inaccurate. This leads to navigation problems as sometimes what is in OSM is not what the street sign says. Since there are multiple datasets supplied by government agencies with a good license for OSM, we data mine these through conflation to get the best name and reference number.</p> <p>Although most of the fields in these datasets aren't useful for OSM, some are like is it a seasonal road, various off road vehicle access permissions, etc... since this is also useful for navigation. Any tags added or edited will follow the OSM Tagging Guidelines for forest roads.</p>"},{"location":"highways/#the-datasets","title":"The Datasets","text":"<p>The primary source of these datasets is available from the  FSGeodata Clearinghouse, which is maintained by the USDA.</p> <p>The Topographical map vector tiles are available from here., which is maintained by the National Forest Service.</p> <p>These have been partially imported in some areas in the past, complete with the bugs in the original datasets. One big advantage though is that the geometry in OSM was from the same USDA datasets at some point in the past, so it's relatively easy to match the geometries. Conflation then is mostly working through the name and reference fields between multiple files, which sometimes don't agree on the proper name.</p> <p>And OpenStreetMap of course.</p>"},{"location":"highways/#processing-the-datasets","title":"Processing The Datasets","text":"<p>Since the files are very large with different schema, a critical part of the conflation process is preparing the data. Some of these files are so large neither QGIS or JOSM can load them without crashing. I use two primary tools for splitting up the files. ogr2ogr for the GeoJson files, and osmium for the OSM XML files. The OSM XML format is required if you want the conflation process to merge the tags into an existing feature. If conflating with OSM data using the GeoJson format, you need to manually cut &amp; paste the new tags onto the existing feature.</p> <p>As you further reduce large datasets to smaller more manageable pieces, this can generate many files. The top level choice is the largest category. I use National Forests boundaries as they can cross state lines.</p> <p>All of the datasets have issues with some features lacking a geometry. These appear to be duplicates of a Feature that does have a good geometry. They are also in \"NAD 83 - EPSG:4269\" for the CRS, so need to convert and fix the geometries. I use ogr2ogr to convert the GDB files to GeoJson like this:</p> <pre><code>ogr2ogr Road_MVUM.geojson S_USA_Road_MVUM.gdb.zip -makevalid -s_srs EPSG:4269 -t_srs EPSG:4326 -sql 'SELECT * FROM Road_MVUM WHERE SHAPE IS NOT NULL'\n\nogr2ogr Trails_MVUM.geojson S_USA_Trail_MVUM.gdb.zip -makevalid -s_srs EPSG:4269 -t_srs EPSG:4326 -sql 'SELECT * FROM Trail_MVUM WHERE SHAPE IS NOT NULL'\n</code></pre> <p>This generates a clean GeoJson file. It has many fields we don't want, so I run a simple conversion program that parses the fields are defined in the original file, and converts the few fields we want for conflation into the OSM equivalent tag/value. For conflation to work really well, all the datasets must use the same schema for the tags and values.</p> <p>Since the MVUM dataset covers the entire country, I build a directory tree in which the deeper you go, the smaller the datasets are. I have the National Forest Service Administrative boundaries unpacked into a top level directory. From there I chop the national dataset into just the data for a forest. This is still a large file, but manageable to edit. Sometimes with rural highway mapping, a large area works better. If there are plans to use the Tasking Manager, The files are still too large, as TM has a 5000sq km limit.</p> <p>Next is generating the task boundaries for each national forest that'll be under the 5000km limit. I used the tm-splitter.py program in this project to use the national forest boundary and break it into squares, and clipped properly at the boundary. These task boundary polygons can then be used to create the project in the Tasking Manager, which will further split that into the size you want for mapping.</p> <p>Something to be conscious of is these external datasets are also full of obscure bugs. Some of the data I think hasn't been updated since the government discovered digital mapping a few decades ago. The conversion utilities will handle all of these problems in these datasets.</p>"},{"location":"highways/#the-openstreetmap-extract","title":"The OpenStreetMap Extract","text":"<p>This step is unnecessary if you plan to manually conflate with a GeoJson file, so jump ahead to the next section.</p> <p>To conflate against OSM data with the goal of automatically merging the tags into the feature you have to prepare the dataset. Each feature needs to be validated anyway, merging tags is more efficient than cut &amp; paste. Since this project is processing data from multiple US states, it exceeds the Overpass data size.</p> <p>I download the states I want to conflate from Geofabrik, and then use osmium merge to turn it into one big file. I have to do this because most of the national forest cross state lines. You'll get duplicate ID errors if you download these files on different days, so grab all the ones you plan to merge at the same time. Geofabrik updates every 24 hours.</p> <p>When dealing with files too large for JOSM or QGIS, osmium is the tool to use. There is also osmfilter and osmconvert which can be used as well. Ogr2ogr can't be used as it can't write the OSM XML format. To merge multiple files with osmium, do this:</p> <pre><code>osmium merge --overwrite -o outdata.osm *.osm.pbf\n</code></pre> <p>The next step is to delete everything but highways from the OSM XML file. When conflating highways, we don't care about amenities or waterways.</p> <p>The prefered data extraction program for conflation is the osmhighways.py program, which has much more fine-grained control, and also replaces the older fixname.py program and fixes the issues when the name field is actually a reference. It also deletes the extraneous tiger:* tags to reduce bloat.</p> <p>You can do something similar with osmium tool, but you wind up with extra features and tags which impacts conflation performance.</p> <pre><code>osmium tags-filter --overwrite --remove-tags -o outdata.osm indata.osm w/highway=track,service,unclassified,primary,tertiary,secondary,path,residential,abandoned,footway,motorway,trunk\n</code></pre> <p>Finally I clip this large file into separate datasets, one for each national forest.</p> <pre><code>osmium extract --overwrite --polygon boundary.geojson -o outdata-roads.osm\n</code></pre> <p>Then the real fun starts after the drudgery of getting ready to do conflation.</p> <p></p>"},{"location":"highways/#forest-road-names","title":"Forest Road Names","text":"<p>The names and reference number in OSM now have a wide variety of incorrect tagging when it comes to names. \"Forest Service Road 123.4A\" is not a name, it is a reference number. Same for \"County Road 43\".  The fixname.py utility scan the OSM extract and when it see incorrect tagging, correct it to the OSM standard. Since the external datasets already follow the same guidelines, this increases the chance of a good match when conflating, since comparing names is part of the process.</p>"},{"location":"highways/#forest-road-reference-numbers","title":"Forest Road Reference Numbers","text":"<p>I'm a huge believer that the name and reference number in OSM should match the street sign, since that's often what is used for navigation. Unfortunately the MVUM data has many highways with a .1 suffix, which some street signs don't display. Also, depending on the age of the paper maps or digital files, older maps lack the .1 suffix, but newer datasets so have the .1 suffix. Since a .1 suffix may be a spur road of questionable quality, it's an important detail, so included when updating the reference numbers.</p> <p>A minor note, the USGS Topographical basemap for JOSM also sometimes lacks the .1 suffix, so can't be used to validate it.</p>"},{"location":"highways/#tiger-tag-deletion","title":"TIGER Tag Deletion","text":"<p>Since there is community consensus that the tiger: tags added back in 2008 when the TIGER data was imported are meaningless, so should be deleted as bloat. The fixnames.py utility used for correct the name also deletes these from each feature so you don't have to manually do it.</p>"},{"location":"highways/#mvum-roads","title":"MVUM Roads","text":"<p>This is all the highways in National Forests. The data contains several fields that would be useful in OSM. This dataset has a grading of 1-5 for the type of vehicle that can drive the road, as well as a field for high clearance vehicles only. This is roughly equivalent to the smoothness tag in OSM. The surface type is also included, which is the same as the OSM surface tag. There are other fields for seasonal access, and seasonal road closures. Roads tagged as needing a high clearance vehicle generate a 4wd_only tag for OSM.</p> <p>The reference numbers often have a typo, an additional number (often 5 or 7) prefixed to the actual number in the original dataset, and were imported this way. Since the reference number needs to match what the map or street sign says, these all need to be fixed. And there are thousands of these...  </p> <p>The type of vehicle that can be driven on a particular road is a bit subjective based on ones off-road driving experience. These are typically jeep trails of varying quality, but very useful for back-country rescues or wildland fires.</p>"},{"location":"highways/#mvum-trails","title":"MVUM Trails","text":"<p>These are Multi Vehicle Use Maps (MVUM), which define the class of vehicle appropriate to drive a road. The trails dataset contains additional highways, as some hiking trails are also forest service roads. These are primarily for hiking, but allow vehicle use, primarily specialized off-road vehicles like an ATV or UTV. They suffer from the same bad data as the MVUM roads.</p>"},{"location":"highways/#national-forest-trails","title":"National Forest Trails","text":"<p>This dataset is hiking trails that don't allow any vehicle usage at all. Many of these trails are in OSM, but lack the trail name and reference number. These also get used for emergency response as well. If there is a name and reference number for the trail, this makes it easier to refer a location to somebody over a radio instead of GPS coordinates.</p>"},{"location":"highways/#usgs-topographical-maps","title":"USGS Topographical maps","text":"<p>It's possible to download the vector datasets used to produce topographical maps. Each file covers a single 7.5 map quad, which is 49 miles or 78.85 km square. There are two variants for each quad, a GDB formatted file, and a Shapefile formatted file. The GDB file contains all the data as layers, whereas the Shapefiles have separate files for each feature type. I find the smaller feature based files easier to deal with. The two primary features we want to extract are Trans_RoadSegment and Trans_TrailSegment. Because of the volume of data, I only have a few states downloaded.</p> <p>I then used ogrmerge to produce a single file for each feature from all the smaller files. This file covers an entire state. This file has also has many fields we don't need, so only want the same set used for all the datasets. The usgs.py contained in this project is then run to filter the input data file into GeoJson with OSM tagging schema. The topographical data is especially useful for conflation, since the name and reference number match the paper or GeoPDF maps many people use.</p> <p>I found a few problems processing the ShapeFiles due to font encoding issues, and also with converting directly to GeoJson. I do this as a two step process, first make a unified ShapeFile from all the other ShapeFiles, and then convert it to GeoJson, which seems to work best.</p> <pre><code>ogrmerge.py -nln highways -single -o highways.shp VECTOR_*/Shape/Trans_Road*.shp -lco ENCODING=\"\"\nogr2ogr highways.geojson highways.shp\n</code></pre>"},{"location":"highways/#conflation","title":"Conflation","text":"<p>Once all the datasets are broken into manageable pieces, and everything is using the OSM tagging schema conflation can start. There are two datasets specified, one is the primary, and the other is the secondary. The tag values in the primary will override the values in the secondary file. To be paranoid about the details, when a tag value is overwritten by the primary data source, the current value becomes old_, ie... name becomes old_name, and then name is updated to the current value. Sometimes when editing the difference in the names is due to abbreviations being used, spelling mistakes, etc... so the old_name can be deleted.</p> <p>When conflating multiple datasets, those need to be conflated against each other before conflating with OSM. Since the topographical dataset is what matches a paper map, or GeoPDF, I consider that the primary dataset. The MVUM and trail data are particularly full of mistakes. Sometimes one dataset has a name, and the other doesn't, so conflation here produces that value.</p> <p>There are also many, many highways in these areas that in OSM only have highway=something. These are easy to conflate as you are only adding new tags. While in TIGER there are many highway=residential, that should really be highway=unclassified or highway=track, it is entirely possible it is a residential road. There's a lot of nice cabins way out in most national forests. But this is the type of thing you'd really need to ground-truth, and luckily doesn't effect navigation when you are out in a network of unmaintained dirt roads.</p> <p></p> <p>The conflation algorithm is relatively simple at the high level, just find all other highways within a short distance, and then check the slope to eliminate a side road that may be touching. At the lower level, there is a lot of support for dealing with the bugs in the external datasets.</p> <p>The conflation algorithm is relatively simple at the high level, just find all other highways within a short distance, and then check the slope to eliminate a side road that may be touching. At the lower level, there is a lot of support for dealing with the bugs in the external datasets.</p>"},{"location":"highways/#editing-in-josm","title":"Editing in JOSM","text":"<p>Unfortunately manually validating the data is very time consuming, but it's important to get it right. I use the TODO plugin and also a data filter so I just select highways. With the TODO plugin, I add the selected features, ideally the entire task. Then I just go through all the features one at a time. When the OSM XML dataset is loaded, nothing will appear in JOSM. This is because the OSM XML file produced by conflation has the refs for the way, but lack the nodes. All it takes is selecting the update modified menu item under the File menu and all the nodes get downloaded, and the highways appear.</p> <p>I often have the original datasets loaded as layers, since sometimes it's useful to refer back to when you find issues with the conflation. Much of the existing data in OSM has many unused tags added during the TIGER import. These also get deleted as meaningless bloat. Some were imported with all the tags from the original dataset which also get deleted. This is life as a data janitor...</p> <p>Once you've validated all the features in the task, it can be run through the JOSM validator, and if all is good, uploaded to OSM. Often the JOSM validator finds many existing issues. I fix anything that is an error, and mostly ignore all the warning as that's a whole other project.</p> <p>If you are editing with the OSM XML file produced by conflation, when the file is opened, there will be some conflicts. This is usually due to things like the incorrect forest road name getting deleted, since now it's a proper ref:usfs reference number. And the tiger tags are gone as well if the fixnames.py utility is used.</p> <p>To fix the conflicts, I just select them all, and click on resolve to my version. Since all the new tags and old tags are preserved, you can edit them directly in the tags window in JOSM. Then I load all the ways into the TODO plugin. You can also use the conflict dialog box to edit the merged tags, but I find the other way more efficient.</p> <p>Using the plugin to validate a feature all I have to do is click on the entry. Sometimes there will be issues that need to be manually fixed. If conflation has changed the name, the old one is still in the</p> <p>feature so a manual comparison can be done. Often validating a feature is just deleting a few tags. But this is the important detail for machine editing. Somebody (not AI) must manually validate each changed feature. This is why the efficiency of mapping is important if you want to update a large area, like an entire national forest.</p> <p>Sometimes there are weird typos that have slipped through the process. This is where the time goes since you have to manually edit the values. But many times for these remote highways you can just mark it as done, and go on to the next one. Many of these highways in OSM have no tags beyond highway=track, so mo conflicts.This lets you validate a large number of features relatively quickly without sacrificing quality.</p>"},{"location":"highways/#editing-osm-xml","title":"Editing OSM XML","text":"<p>The conflation process produces an output file in OSM XML format. This file has incremented the version number and added action=modify to the attributes for the feature. When loaded into OSM, no data is initially visible. If you go to the File menu, go down and execute update modified. This will download all the nodes for the ways, and all the highways will become visible. Highways that have multiple tags already in OSM will become a conflict. These can be resolved easier in JOSM using the conflict dialog box. No geometries have changed, just tags, so you have to manually select the tags to be merged. Features without tags beyond highway=something merge automatically. which makes validating these features quick and easy. Note that every feature needs to be validated individually.</p>"},{"location":"highways/#editing-geojson","title":"Editing GeoJson","text":"<p>While JOSM can load and edit GeoJson data, not being in a native OSM format it can't be automatically merge. Instead load the GeoJson file and then create a new OSM layer. I select all the highways in the task, and load them into the TODO plugin. Sometimes there are so few highways, I don't use the TODO plugin. I then cut the tags and values for a feature from the GeoJson file, then switch to the OSM layer, and paste the tags into the feature.</p>"},{"location":"highways/#validating","title":"Validating","text":"<p>Here's an example of the results of a 3 way conflation. This was between the MVUM data, the topographical data, and OSM data.</p> <ul> <li>highway=unclassified</li> <li>lanes=2</li> <li>name=Whisky Park Road</li> <li>operator=US Forest Service</li> <li>ref:usfs=FR 503</li> <li>smoothness=good</li> <li>surface=gravel</li> </ul> <p>Note that the name is spelled wrong. Here is a document on how to validating in JOSM for much more detail.</p>"},{"location":"highways/#splitting-highways","title":"Splitting Highways","text":"<p>In national forest lands, the reference number changes at every major intersection. Side roads that branch off have an additional modifier added. or example, the main road may be called ref:usfs=\"FR 505\", with a change to ref:usfs=\"FR 505.1\" when it crosses a state line. Spur roads (often to campsites) get a letter attached, so the spur road is *ref:usfs=\"FR 505.1A\". Understanding how the reference numbers are assigned makes it easy to transmit your location over a radio or phone, and have somebody looking on a map find that location. Much easier than using GPS coordinates.</p> <p>For the highways that were traced off of satellite imagery, there is often a problem with forks in the road. Often tree cover or poor resolution imagery makes it hard to see the highway. And a lot of the highways go through an area with an entire network of other dirt roads, so the reference number may just group a bunch of highway segments. Often the most visible highway branch in the imagery at a fork is not the actual road. In this case the highway has to be split at the fork, and the new segment tagged for it's actual value, and the actual highway segment gets tagged correctly. This is critical if you want navigation to work.</p>"},{"location":"highways/#ground-truthing","title":"Ground-truthing","text":"<p>If you really want detailed and accurate maps, ground-truthing is an important part of the process. Road conditions change, especially the unmaintained dirt roads. Years of erosion, off-road vehicle abuse, etc... all change. For this reason the surface, smoothness and tracktype tags are not merged, as what is in the external datasets is likely out of date. Also sometimes parts of a dirt road get paved, or access is closed off completely.</p> <p>This is a good excuse to go there for some hiking and camping fun. You can load data into StreetComplete when online, and then use that in the field since will likely be no cell phone connection. Depending on the software used to collect the data, that may need conflation before uploading, for example OpenDataKit data. Some detail on that process is in this Highway Mapping blog post about a field mapping trip.</p>"},{"location":"mvum/","title":"MVUM &amp; RodCore Conversion","text":"<p>The MVUM and RoadCore datasets are all of the motor vehicle roads in a national forest. These are primarily remote dirt roads, often just a jeep track. These are heavily used for back country access for wildland fires and rescues. Currently much of this data has been imported in the past, complete with all the bugs in the dataset. The RoadCore datasets is a superset of the MVUM data, but otherwise the same.</p> <p>This utility program normalizes the data, correcting or flagging bugs as an aid for better conflation. It can process both the MVUM and RoadCore datasets.</p> <p>The original datasets can be found here on the USDA  FSGeodata Clearinghouse website.</p>"},{"location":"mvum/#dataset-bugs","title":"Dataset Bugs","text":""},{"location":"mvum/#bad-geometry","title":"Bad Geometry","text":"<p>There are many instances where a highway in the MVUM or RoadCore data is a MultiLineString instead of just a LineString. The problem with these are sometimes the segments are far apart, with long sections with no data. These are all the same highway, just bad data. To me it looks like somebodies's GPS had a dropped signal in places when they were recording a track.</p>"},{"location":"mvum/#bad-reference-numbers","title":"Bad Reference Numbers","text":"<p>In some areas the MVUM and RoadCore data has extract numerals prefixed to the actual reference number. These are all usually in the same area, so I assume whomever was doing data entry had a sticky keyboard, it got messed up when converting from paper maps to digital, who really knows. But it makes that tag worthless. Utah datasets in particular suffer greatly from this problem.</p> <p>Another common problem in the reference nummbers is in some areas the major maintained roads have a .1 appended. All minor part of the number should always have a letter appended. So FR 432.1\" is actually *FR 432\", whereas \"432.1A is correct. This was confirmed by reviewing multiple other map sources, as the paper and PDF version of the dataset has the correct version without the .1 appended. Obviously this dataset is not used to produce the maps you can get from the Forest Service.</p> <p>Cleaning up all the wrong reference numbers will make OSM the best map for road and trail navigation on public lands.</p>"},{"location":"mvum/#dixie-national-forest","title":"Dixie National Forest","text":"<p>In the current MVUM dataset for this national forest, for some reason a 30, 31, 32, 33, 34 has been prefixed to many of the IDs, making the reference numbers wrong. After staring at the original data file, I noticed these were all 5 characters long, and lacked a letter or a minor number suffix. Limiting the trigger to just that case seems to fix the problem. A note is added to any feature where the ref:usfs is changed as an aid towards validation.</p>"},{"location":"mvum/#manti-lasal-national-forest","title":"Manti-LaSal National Forest","text":"<p>In the current MVUM dataset for this national forest, for some reason a 5 or 7 has been prefixed to many of the IDs, making the reference numbers wrong.</p>"},{"location":"mvum/#fishlake-national-forest","title":"Fishlake National Forest","text":"<p>In the current MVUM dataset for this national forest, for some reason a 4 or 40 has been prefixed to some of the IDs, making the reference numbers wrong.</p>"},{"location":"mvum/#mount-hood-national-forest","title":"Mount Hood National Forest","text":"<p>For some reason, some of the reference numbers have a 000 appended, making the reference numbers wrong. This applies to paved roads, not just remote jeep tracks.</p>"},{"location":"mvum/#doesnt-match-the-sign","title":"Doesn't Match The Sign","text":"<p>There is an issue with the USFS reference numbers not matching the sign. This is luckily limited to whether there is a .1 appended to the reference number without an letter at the end. Usually a reference without a .1 is a primary road, and the .1 gets appended for a major branch off that road. While out ground-truthing MVUM roads recently I saw multiple examples where the reference numnber in the MVUM data (and often in OSM) has the .1, so I use that value regardless of what the sign says. It's still quite obviously what the reference number is since the only difference is the .1 suffix.</p> <p>This gets more interesting when you compare with other data sources, ie... paper and digital maps. Older data source seem to drop the .1, whereas the same road in a newer version of the dataset has the .1 suffix. So I figure anyone navigating remote roads that checks their other maps would figure out which way to go. So anyway, when way out on remote very_bad or horrible MVUM roads, you should have multiple maps if you don't want to get confused.</p>"},{"location":"mvum/#missing-geometry","title":"Missing Geometry","text":"<p>There are features with no geometry at all, but the tags all match an existing feature that does have a geometry. These appear to be accidental duplicates, so they get removed.</p>"},{"location":"mvum/#dropped-fields","title":"Dropped Fields","text":"<p>These fields are dropped as they aren't useful for OpenStreetMap.</p> <ul> <li>TE_CN</li> <li>BMP</li> <li>EMP</li> <li>SYMBOL_CODE</li> <li>SEG_LENGTH</li> <li>JURISDICTION</li> <li>SYSTEM</li> <li>ROUTE_STATUS</li> <li>OBJECTIVE_MAINT_LEVEL</li> <li>FUNCTIONAL_CLASS</li> <li>LANES</li> <li>COUNTY</li> <li>CONGRESSIONAL_DISTRICT</li> <li>ADMIN_ORG</li> <li>SERVICE_LIFE</li> <li>LEVEL_OF_SERVICE</li> <li>PFSR_CLASSIFICATION</li> <li>MANAGING_ORG</li> <li>LOC_ERROR</li> <li>GIS_MILES</li> <li>SECURITY_ID</li> <li>OPENFORUSETO</li> <li>IVM_SYMBOL</li> <li>GLOBALID</li> <li>SHAPE_Length</li> </ul>"},{"location":"mvum/#preserved-fields","title":"Preserved Fields","text":"<p>The field names are a bit truncated in the dataset, but these are the ones that are converted. The MVU and RoaCore datasets uses the same columns names, the only difference is whether an underbar is used.</p> <ul> <li>ID is id</li> <li>NAME is name</li> <li>OPER_MAINT_LEVEL is smoothness</li> <li>SYMBOL_NAME smoothness</li> <li>SURFACE_TYPE is surface</li> <li>SEASONAL is seasonal</li> <li>PRIMARY_MAINTAINER is operator</li> </ul>"},{"location":"mvum/#abbreviations","title":"Abbreviations","text":"<p>There are multiple and somewhat inconsistent abbreviations in the MVUM dataset highway names. OpenStreetMap should be using the full value. These were all found by the conflation software when trying to match names between two features. Since much of the MVUM data is of varying quality, there's probably a few not captured here that will have to be fixed when editing the data. This however improves the conflation results to limit manual editing.</p> <ul> <li>\" Cr \" is \" Creek \"</li> <li>\" Cr. \" is \" Creek \"</li> <li>\" Crk \" is \" Creek \"</li> <li>\" Cg \" is \" Campground \"</li> <li>\" Rd. \" is \" Road\"</li> <li>\" Mt \" is \" Mountain\"</li> <li>\" Mtn \" is \" Mountain\"</li> <li>\" Disp \" is \" Dispersed\"</li> <li>\" Rd. \" is \" Road\"</li> <li>\" Mtn. \" is \" Mountain\"</li> <li>\" Mtn \" is \" Mountain\"</li> <li>\" Lk \" is \" Lake\"</li> <li>\" Resvr \" is \" Reservoir\"</li> <li>\" Spg \" is \" Spring\"</li> <li>\" Br \" is \" Bridge\"</li> <li>\" N \" is \" North\"</li> <li>\" W \" is \" West\"</li> <li>\" E \" is \" East\"</li> <li>\" S \" is \" South\"</li> <li>\" So \" is \" South\"</li> </ul>"},{"location":"mvum/#tag-values","title":"Tag values","text":""},{"location":"mvum/#oper_maint_level","title":"OPER_MAINT_LEVEL","text":"<p>This field is used to determine the smoothness of the highway. Using the official forest service guidelines for this field, convienently they publish a Road Maintaince Guidelines, complete with muiltiple pictures and detaild technical information on each level. The coorelate these values, I did some ground-truthing on MVUM and I'd agree that level 2 is definetely high clearance vehicle only, and that it fits the definition here for very_bad, although some sections were more horrible, deeply rutted, big rocks, lots of erosion.</p> <ul> <li> <p>5 -HIGH DEGREE OF USER COMFORT:  Assigned to roads that provide a high degree of user comfort and convenience. This becomes smoothness=excellent.</p> </li> <li> <p>4 -MODERATE DEGREE OF USER COMFORT:  Assigned to roads that provide a moderate degree of user comfort and convenience at moderate travel speeds. This becomes smoothness=bad.</p> </li> <li> <p>3 -SUITABLE FOR PASSENGER CARS:  Assigned to roads open for and maintained for travel by a prudent driver in a standard passenger car. This becomes smnoothness=good.</p> </li> <li> <p>2 -HIGH CLEARANCE VEHICLES:  Assigned to roads open for use by high clearance vehicles. This adds 4wd_only=yes and becomes smoothness=vary_bad.</p> </li> <li> <p>1 -BASIC CUSTODIAL CARE (CLOSED):  Assigned to roads that have been placed in storage (&gt; one year) between intermittent uses. Basic custodial maintenance is performed. Road is closed to vehicular traffic. This becomes access=no</p> </li> </ul>"},{"location":"mvum/#symbol_name","title":"SYMBOL_NAME","text":"<p>Sometimes OPER_MAINT_LEVEL doesn't have a value, so this is used as a backup. These values are not used to update the existing values in OSM, they are only used for route planning ground-truthing trips.</p> <ul> <li>Gravel Road, Suitable for Passenger Car becomes surface=gravel</li> <li>Dirt Road, Suitable for Passenger Car becomes surface=dirt</li> <li>Road, Not Maintained for Passenger Car becomes smoothness=very_bad</li> <li>Paved Road becomes surface=paved</li> </ul>"},{"location":"mvum/#surface_type","title":"SURFACE_TYPE","text":"<p>This is another field that is converted, but not used when editing the existing OSM feature. This can only really be determined by ground-truthing, but it converted as another aid for route planning.</p> <ul> <li>AGG -CRUSHED AGGREGATE OR GRAVEL becomes surface=gravel</li> <li>AC -ASPHALT becomes surface=asphalt</li> <li>IMP -IMPROVED NATIVE MATERIAL becomes surface=compacted</li> <li>CSOIL -COMPACTED SOIL becomes surface=compacted</li> <li>NAT -NATIVE MATERIAL becomes surface=dirt</li> <li>P - PAVED becomes surface=paved</li> </ul>"},{"location":"mvum/#name","title":"Name","text":"<p>The name is always in all capitol letters, so this is converted to a standard first letter of every word is upper case, the rest is lower case.</p>"},{"location":"mvum/#options","title":"Options","text":"<pre><code>-h, --help            show this help message and exit\n-v, --verbose         verbose output\n-i INFILE, --infile INFILE MVUM data file\n-c, --convert         Convert MVUM feature to OSM feature\n-o OUTFILE, --outfile OUTFILE Output GeoJson file\n</code></pre>"},{"location":"odkconflation/","title":"Conflating OpenDataKit with OpenStreetMap","text":"<p>Typically conflation is done when doing data imports, but not always. Data collected in the field can be considered an import. Conflating buildings or POIs from external data is relatively easy as it's already been cleaned up and validated. When you are doing field mapping, then you have to cleanup and validate the data during conflation. This is a time consuming process even with good conflation software.</p> <p>I've worked with multiple conflation software over the years. Hootenanny, OpenJump (later forked into RoadMatcher), etc...  which currently are now dead projects. Conflation is a hard technical challenge and often the results are poor and unstatisfing result. For smalller datasets often it's easier to do do manual conflation using JOSM or Qgis. This project tries to simply the problem by focusing on OpenStreetMap data.</p>"},{"location":"odkconflation/#smartphone-data-collection","title":"Smartphone Data Collection","text":"<p>While commercial organizations may use expensive GPS devices, most of us that do data collection as a volunteer or for an NGO use their smartphone. Their is a variety of smartphone apps for data collection that fall ihnto two categories. The first category are the apps like Vespucci, StreetComplete, and Organic Maps. These directly upload to OpenStreetMap. These are great for the casual mapper who only adds data occasionally and is limited to a POI. For example, a casual mapper may want to add the restaurant they are currrently eating in when they notices it's not in OpenStreetMap. In addition, they probably have a cell phone connection, so the data gets added right away.</p> <p>The other category are apps like ODK Collect, QField ArcGIS Field Maps which are oriented to larger scale mapping projects, often offline without any cellular connection. These collect a lot of data that then needs to get processed later. And conflation is part of this process.</p> <p>All of these smartphone based data collection apps suffer from poor GPS location accuracy. Modern smartphones (2024) are often 5-9 meters off the actual location, sometimes worse. In addition when field data collecting, you can't always record the actual location you want, you can only record where you are standing.</p> <p>You can improve the location data somewhat if you have a good quality basemap, for example you see a building within a courthouse wall when you are standing in the street. If you have a basemap, typically satellite imagery, you can touch the location on the basemap, and use that instead of where you are standing. Then later when conflating, you have a much higher chance the process will be less painful.</p>"},{"location":"odkconflation/#opendatakit","title":"OpenDataKit","text":"<p>OpenDataKit is a format for data import forms used to collect custom data. The source file is a spreadsheet, called an XLSForm. This gets used by the mobile app for the quesion and answer process defined by the XLSForm. There are multiple apps and projects using XLSForms, so it's well supported and maintained.</p> <p>The XLS source file syntax is a bit wierd at first, being a spreadsheet, so the osm-fieldwork project contains tested XLSForm templates for a variety of mapping project goals. These can be used to create efficient XForms that are easy to convert to OSM. The primary task when manually converting ODK collected data into OSM format is converting the tags. If the XLSForm is created with a focus towards OSM the XLSForm can make this a much simpler process. This is detailed more in this document. Simply stated, what is in the name colum in the XLSForm becomes the name of the tag in OSM, and the response from the choices sheet becomes the value.</p>"},{"location":"odkconflation/#odk-collect-central","title":"ODK Collect &amp; Central","text":"<p>ODK Collect is a mobile app for data collection using XLSForms. It's server side is ODK Central, which replaces the  older ODK Aggregate. ODK Central manages the XLSForms downloaded to your phone, as wall as the submissions uploaded from your phone when back online.</p> <p>A related project for processing ODK data and working remotely with Central is osm-fieldwork. This Python project handles conversion of the various data files from Collect or Central, into OSM XML and GeoJson for future processing via editing or conflation. This is heavily used in the FMTM backend.</p>"},{"location":"odkconflation/#field-data-collection","title":"Field Data Collection","text":"<p>Collecting data in the field is to best way to add data to OpenStreetMap. Whether done by casual mappers adding POIs, to more dedicated mappers, what is reality at that moment is the key to keeping OSM fresh and updated. When it comes to improving the metadata for buildings, many have been imported with building=yes from remote mapping using the HOT Tasking Manager to trace buildings from satellite imagery. </p> <p>But ground-truthing what kind of building it is improvers the map. It may be a medical clinic, restaurant, residence, etc.. who know until somebody stands in front of the building to collect more informsation about it. This may be idenifying it as a clinic or reseidence, adding the building material, what is the roof made of, is it's power non-existance, or are there solar panels or a generator ? Some humanitarian mapping is collecting data on public toilets, and community water sources for future improvements. </p> <p>Knowing there is a building on the map is useful, but better yet is what is the building used for ? What is it made of ? Does it have AC or DC power ? Water available ? All of these details improve the map to make it more useful to others.</p>"},{"location":"odkconflation/#field-mapping-tasking-manager","title":"Field Mapping Tasking Manager","text":"<p>The Field Mapping Tasking Manager (FMTM) is a project to oprganize large scale data collection using ODK Collect and ODK Central. It uses the osm-fieldwork project for much of the backend processing of the ODK data,  but is designed for large scale field mapping involving many people. It uses ODK Collect and ODK Central as the primary tools. One of the final steps in processing ODK data to import into OSM is conflating it with existing data. This can be done manually of course, but with a large number of data submissions this becomes tedious and time consuming. FMTM aggrgates all the data for an entire project, and may have thousands of submissions. This is where conflation is critical.</p>"},{"location":"odkconflation/#the-algorythm","title":"The Algorythm","text":"<p>Currently conflation is focused on ODK with OSM. This uses the conflator.py program which can conflate between the ODK data and an OSM data extract. There are other conflation programs in this project for other external datasets, but uses a postgres database instead of two files.</p>"},{"location":"odkconflation/#the-conflator-class","title":"The Conflator() Class","text":"<p>This is the primary interface for conflating files. It has two primary endpoint. This top level endpoint is Conflator.conflateFiles(), which is used when the conflator program is run standalone. It opens the two disk files, parses the various formats, and generates a data structure used for conflation. This class uses the Parsers() class from osm-fieldwork that can parse the JSON or CSV files downloaded from ODK Central, or the ODK XML \"instance\" files when working offline. OPSM XML or GeoJson files are also supported. Each entry in the files is turned into list of python dicts to make it easier to compaert the data.</p> <p>Once the two files are read, the Conflator.conflateFeatures() endpoint takes the two lists of data and does the actual conflation. There is an additional parameter passed to this endpoint that is the threshold distance. This is used to find all features in the OSM data extract within that distance. Note that this is a unit of the earth's circumforance, not meters, so distance calulations are a bit fuzzy.</p> <p>This is a brute force conflation algorythm, not fast but it tries to be complete. it is comprised of two loops. The top level loops through the ODK data. For each ODK data entry, it finds all the OSM features within that threshold distance. The inner loop then uses the closest feature and compares the tags. This is where things get interesting.... If there is a name tag in the ODK data, this is string compared with the name in the closest OSM feature. Fuzzy string matching is used to handle minor spelling differences. Sometimes the mis-spelling is in the OSM data, but often when entering names of features on your smartphone, mis-typing occurs. If there is a 100% match in the name tags, then chances are the feature exists in OSM already.</p> <p>If there is no name tag in the ODK data, then the other tags are compared to try to find a possible duplicate feature. For example, a public toilet at a trailhead has no name, but if both ODK and OSM have amenity=toilet, then it's very likey a duplicate. If no tags match, then the ODK data is proably a new feature.</p> <p>Any time a possible duplicate is found, it is not automatically merged. Instead a fixme tag is added to the feature in the output file with a statement that it is potentially a duplicate. When the output file is loaded into JOSM, you can search for this tag to manually decide if it is a duplicate.</p>"},{"location":"odkconflation/#xlsform-design","title":"XLSForm Design","text":"<p>Part of the key detail to improve conflation requires a carefully created XLSForm. There is much more detailed information on XLSForm design, but briefly whatever is in the name column in the survey sheet becomes the name of the tags, and whatever is in the name column in the choices sheet becomes the value. If you want a relatively smooth conflation, make sure your XLSForm uses OSM tagging schemas.</p> <p>If you don't follow OSM tagging, then conflation will assumme all your ODK data is a new feature, and you'll have to manually conflate the results using JOSM. That's OK for small datasets, but quickly becomes very tedious for the larger datasets that FMTM collects.</p>"},{"location":"odkconflation/#the-output-file","title":"The Output File","text":"<p>The output file must be in OSM XML to enable updating the ways. If the OSM data is a POI, viewing it in JOSM is easy. If the OSM data is a polygon, when loaded into JOSM, they won't appear at first. Since the OSM way created by conflation has preserved the refs used by OSM XML to reference the nodes, doing update modified in JOSM then pulls down the nodes and all the polygons will appear.</p>"},{"location":"odkconflation/#conflicts","title":"Conflicts","text":"<p>There are some interesting issues to fix post conflation. ODK data is usually a single POI, whereas in OSM it may be a polygon. Sometimes though the POI is already in OSM. Remote mapping or building footprint imports often have a polygon with a single building=yes tag. If the POI we collected in ODK has more data, for example this building is a restaurant serving pizza, and is made of brick.</p> <p>In OSM sometimes there is a POI for an amenity, as well as a building polygon that were added at different times by different people.  The key detail for conflation is do any of the tags and values from the new data match existing data ?</p> <p>FMTM downloads a data extract from OSM using osm-rawdata, and then filters the data extract based on what is on the choices sheet of the XLSForm. Otherwise Collect won't launch. Because this data extract does not contain all the tags that are in OSM, it creates conflicts. This problem is FMTM specific, and can be improved by making more complete data extract from OSM.</p> <p>When the only tag in the OSM data is building=, any tags from ODK are merged with the building polygon when possible. If the OSM feature has other tags, JOSM will flag this as a conflict. Then you have to manually merge the tags in JOSM.</p>"},{"location":"osm-merge/","title":"Conflator Program","text":"<p>osm-merge is a program that conflates building footprint data with OpenStreetMap data to remove duplicates. The result of the conflation process is buildings that only exist in the footprints data file.</p> <p>This program can process data from either a postgres database, or data files in geojson, shapefile format. One of the core concepts is using a data file of polygons to filter the larger datasets, since a database may contain multiple countries.</p> <p>The process of setting up for large scale conflation is in this document.</p>"},{"location":"osm-merge/#command-line-options","title":"Command Line Options","text":""},{"location":"osm-merge/#common-options","title":"Common Options","text":"<p>These are the nost commonly used options.</p> <pre><code>--help(-h)       Get command line options\n--verbose(-v)    Enable verbose output\n--boundary(-b)   Specify a multipolygon for boundaries, one file for each polygon\n--project(-p)    Tasking Manager project ID to get boundaries from database\n--osmdata(-x)    OSM XML/PBF or OSM database to get boundaries (prefix with pg: if database)\n--outdir(-o)     Output file prefix for output files (default \"/tmp/tmproject-\")\n--footprints(-f) File or building footprints Database URL (prefix with pg: if database)\n--dbhost(-d)     Database host, defaults to \"localhost\"\n--dbuser(-u)     Database user, defaults to current user\n--dbpass(-w)     Database user, defaults to no password needed\n</code></pre>"},{"location":"osm-merge/#tasking-manager-options","title":"Tasking Manager Options","text":"<p>These options are used to dynamically extract a project boundary from a Tasking Manager database. A more common usage is to use the splitter.py program to download the project boundary from the Tasking Manager itself.</p> <pre><code>--splittasks     When using the Tasking Manager database, split into tasks\n--schema         OSM database schema (pgsnapshot, ogr2ogr, osm2pgsql) defaults to \"pgsnapshot\"\n--tmdata(-t)     Tasking Manager database to get boundaries if no boundary file prefix with pg: for database usage, http for REST API\n</code></pre>"},{"location":"osm-merge/#osm-options","title":"OSM Options","text":"<p>When extracting administrative boundaries from an OpenStreetMap database, the default admin levl is 4, which is commonly used for couty boundaries. This lets the user select what level of administrative boundaries they want.</p> <pre><code>--admin(-a)      When querying the OSM database, this is the admin_level, (defaults to 4)\n</code></pre>"},{"location":"osm-merge/#examples","title":"Examples","text":"<p>PATH/conflator.py -v -x 12057-osm.geojson -f 12057-ms.geojson -o 12057</p> <p>This takes two disk files, which have already been filtered to only contain data for the area to conflate.</p> <p>PATH/conflator.py -v -x pg:kenya -b 12007-project.geojson -f 12057-ms.geojson -o 12057</p> <p>This uses a database that contains all of Kenya, but we only want to process a single project, so that's supplied as the boundary. The foorptin data was already filtered using ogr2ogr, and the project ID is used as the prefix for the output files.</p> <p>PATH/conflator.py -v -x pg:kenya -b 12007-project.geojson -f pg:kenya_footprints -o 12057 -d mapdb -u me</p> <p>This is the same except the database is on a remote machine called mapdb and the user needs to be me.</p> <p>PATH/conflator.py -t tmsnap -p 8345 -b pg:kenya_foot -o pg:Kenya</p> <p>Reads from 3 data sources. The first one is a snapshot of the Tasking Manager database, and we want to use project 8345 as the boundary. The two data sources are prefixed with \"pg\", which defines them as a database URL instead of a file. The database needs to be running locally in this case.</p>"},{"location":"osmhighways/","title":"OpenStreetMap Data","text":"<p>Being crowd sourced and open to all who want to contribute, OpenStreetMap (OSM) has infinite flexibility in the various tag/values used for metadata. Many of the tags not in common use are ignored by the renderers and routing engines, but still live in the database and data files. You'd really only notice if you're deep in the data, which is the key to good conflation.</p> <p>The features in OSM come from a wide variety of sources. Mobile apps, imports, satellite imagery. Often features traced from imagery are lacking any tags beyond building=yes or highway=track, which we hope to improve on by conflating with other datasets.</p>"},{"location":"osmhighways/#data-janitor","title":"Data Janitor","text":"<p>Being a data janitor is important, if rather boring and tedious task. Bugs in the data can lead to navigation problems at the very least. An accurate and detailed map is a thing of beauty, and often OSM gets really close.</p> <p>Unfortunately to conflate OSM data with external data sources, it needs to be cleaned up. Normally it gets cleaned up by the mapper, who has to manually review and edit the tags. Since the highway name is an important item used to confirm a near match in geometry, too much variety can make this a slow process.</p> <p>This project has an osmhighways.py program that is used to cleanup some of the problems, like deleting unnecessary tags, and fixing the name vs reference number problem. Deleting all bogus tags reduces the data size, which is a benefit. This project also extracts only highway linestrings, so a clean dataset for conflating geometries.</p>"},{"location":"osmhighways/#old-imports","title":"Old Imports","text":"<p>OpenStreetMap (OSM) has a past history of imports, often done way back when OSM had little highway data. This was a way to bootstrap navigation, and it mostly worked. </p>"},{"location":"osmhighways/#tiger","title":"TIGER","text":"<p>Since it was publically available, the data used by the US Census Department was imported around 2007. The data is of varying quality, but was better than nothing. The OSM community has been cleaning up the mess ever since. More information on the TIGER fixup can be found  here.</p> <p>An small example of the tags added from TIGER, all of which can be deleted.</p> <pre><code>    &lt;tag k=\"tiger:name_base\" v=\"75th\"/&gt;\n    &lt;tag k=\"tiger:name_base_1\" v=\"75th\"/&gt;\n    &lt;tag k=\"tiger:name_direction_prefix\" v=\"N\"/&gt;\n    &lt;tag k=\"tiger:name_type\" v=\"St\"/&gt;\n    &lt;tag k=\"tiger:name_type_1\" v=\"St\"/&gt;\n    &lt;tag k=\"tiger:cfcc\" v=\"A41\"/&gt;\n    &lt;tag k=\"tiger:reviewed\" v=\"no\"/&gt;\n</code></pre> <p>I don't think I've ever seen a tiger:reviewed=yes tag.</p>"},{"location":"osmhighways/#motor-vehicle-use-map-mvum","title":"Motor Vehicle Use Map (MVUM)","text":"<p>The MVUM data is highways in national forests, so useful in remote area not always in TIGER. Or in TIGER but completely wrong. I've seen roads in TIGER that don't actually exist. All the MVUM data is better quality as much of the data was mapped by ground-truthing. It has useful data fields, like is a high clearance vehicle needed, what is the surface, and other access data like are ATVs allowed ?</p> <p>[MVUM)](https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.Road_MVUM.zip</p>"},{"location":"osmhighways/#clipping","title":"Clipping","text":"<p>To support conflation, even OSM data needs to be chopped into smaller pieces. While osmium and osmfilter could so this, I've had problmes with the other tools when the task polygon is small. The osmhighways.py program also clips files. Since it's in an OSM data format, we can't really use shapely, or geopandas, just osmium. It's a bit slow, being pure python. If it's a continuing problem I'll refactor it into C++.</p> <p>There is a question as to whether it's better to clip a highway at the bundary, or include ther part of it's geometry tha's outside the boundary. I'm experimenting with both, and seeing how it effects conflation.</p>"},{"location":"osmhighways/#options","title":"Options","text":"<pre><code>-h, --help                     show this help message and exit\n-v, --verbose                  verbose output\n-i INFILE, --infile INFILE     Input data file\n-o OUTFILE, --outfile OUTFILE  Output filename\n-c CLIP, --clip CLIP           Clip data extract by polygon\n-s SMALL, --small SMALL        Small dataset\n</code></pre> <p>This program extracts all the highways from an OSM file, and correct as many of the bugs with names that are actually a reference number.</p> <pre><code>For Example:\n    osmhighways.py -v -i colorado-latest.osm.pbf -o co-highways.osm\n</code></pre>"},{"location":"tm-splitter/","title":"TM Splitter Utility","text":"<p>This is a simple utility for task splitting to reduce large datasets into more manageble sizes. It is oriented towards generating a MultiPolyon or Polygons which can then be be used by ogr2ogr or osmium for making data extracts.</p>"},{"location":"tm-splitter/#administrative-boundaries","title":"Administrative Boundaries","text":"<p>The administrative boundary files are a MultiPolygon of every national forest, park, or wilderness aea in the United States. Using the --split option creates a MultiPolygon of each region that can be used to make data extracts using ogr2ogr or osmium. Each output file has the name of the region as the filename.</p>"},{"location":"tm-splitter/#grid-creation","title":"Grid Creation","text":"<p>Once the files for each region have been generated, they are still large. Next a grid can be generated from each region as a MultiPloygon. Each task in the grid is the maximum size supported to create a Tasking Manager project, which is 5000square km.</p>"},{"location":"tm-splitter/#task-creation","title":"Task Creation","text":"<p>To create a file for making data extracts, the grid can be further split into indivigual files for to use with ogr2ogr or osmium.</p>"},{"location":"tm-splitter/#options","title":"Options","text":"<p>Usage: tm-splitter -h -g [-s] -i INFILE [-o OUTFILE]</p> <p>options:   -h, --help                             show this help message and exit   -v, --verbose                          verbose output   -s, --split                            Split Multipolygon   -g, --grid                             Create a grid from an AOI   -m, --meters                           Square area in meters   -i INFILE, --infile INFILE             The input dataset   -o OUTFILE, --outfile OUTFILE          Output filename</p>"},{"location":"trails/","title":"National Park Service Trails","text":"<p>This processes both the National Park Service trails dataset, and the National Forest Service trail datasets. The schema of the two datasets is very similar. One of the differences is for Park Service Trails has two default tags in the output file which are bicycle=no and motor_vehicle=no. These default tags are documented here.</p> <p>This dataset is available in a variety of formats from the ArcGIS Hub.</p>"},{"location":"trails/#processed-fields","title":"Processed Fields","text":"<p>These are the fields extracted from the data that are converted to OpenStreetMap syntax so they can be conflated.</p> <ul> <li>OBJECTID becomes id</li> <li>TRLNAME becomes name</li> <li>TRLCLASS becomes sac_scale</li> <li>TRLUSE becomes yes for horse, bicycle, atv, etc...</li> <li>TRLALTNAME becomes alt_name</li> <li>SEASONAL becomes seasonal</li> <li>MAINTAINER becomas operator</li> <li>TRLSURFACE becomes surface</li> </ul>"},{"location":"trails/#dropped-fields","title":"Dropped Fields","text":"<p>These fields are all ignored, and are dropped from the output file.</p> <ul> <li>MAPLABEL</li> <li>TRLSTATUS</li> <li>TRLTYPE</li> <li>PUBLICDISP</li> <li>DATAACCESS</li> <li>ACCESSNOTE</li> <li>ORIGINATOR</li> <li>UNITCODE</li> <li>UNITNAME</li> <li>UNITTYPE</li> <li>GROUPCODE</li> <li>GROUPNAME</li> <li>REGIONCODE</li> <li>CREATEDATE</li> <li>EDITDATE</li> <li>LINETYPE</li> <li>MAPMETHOD</li> <li>MAPSOURCE</li> <li>SOURCEDATE</li> <li>XYACCURACY</li> <li>GEOMETRYID</li> <li>FEATUREID</li> <li>FACLOCID</li> <li>FACASSETID</li> <li>IMLOCID</li> <li>OBSERVABLE</li> <li>ISEXTANT</li> <li>OPENTOPUBL</li> <li>ALTLANGNAM</li> <li>ALTLANG</li> <li>NOTES</li> </ul>"},{"location":"trails/#national-forest-service-trails","title":"National Forest Service Trails","text":"<p>The US Forest Service makes much of their data publically accessible, so it's been a source for imports for a long time. There is a nice detailed wiki page on the Forest Service Data. The conversion process handles most of the implementation details.</p>"},{"location":"trails/#keep-fields","title":"Keep Fields","text":"<p>The two primary fields are TRAIL_NO, which is used for the ref:usfs tags, and TRAIL_NAME, which is the name of the trail. In addition to these</p>"},{"location":"trails/#the-5-variations","title":"The 5 Variations","text":"<p>For many of the features classes, there are 5 variations on each one which is used for access.</p> <ul> <li>Managed: Usage allowed and managed by the forest service</li> <li>Accepted: Usage is accepted year round</li> <li>Accepted/Discouraged: Usage is accepted, but discouraged</li> <li>Restricted: Usage is restricted</li> <li>Discouraged: Usage is discouraged</li> </ul> <p>These are converted to the apppropriate value.</p> <ul> <li>Managed* sets the keyword to designated</li> <li>Accepted* sets the keyword to yes</li> <li>Restricted* sets the keyword to no</li> <li>Discouraged* sets the keyword to discouraged</li> <li>Accepted/Discouraged* sets the keyword to permissive</li> </ul> <p>Many of the values for these are NULL, so ignored when generating the output file. If the value exists, it's either a Y or a N, which is used to set the values. For example: \"SNOWMOBILE\": \"Y\" becomes snowmobile=yes in the output file.</p> <ul> <li>PACK_SADDLE_ becomes horse=</li> <li>BICYCLE_ becomes bicycle=</li> <li>MOTORCYCLE_ becomes motorcycle=</li> <li>ATV_ becoms atv=</li> <li>FOURWD_ becomes 4wd_only</li> <li>SNOWMOBILE_ becomes snowmobile=</li> <li>SNOWSHOE_ becomes snowwhoe=</li> <li>XCOUNTRY_SKI_ becomes ski</li> </ul> <p>Currently these fields appear to be empty, but that may change in the future.</p> <ul> <li>SNOWCOACH_SNOWCAT_</li> <li>SNOWCOACH_SNOWCAT_</li> <li>E_BIKE_CLASS1_</li> <li>E_BIKE_CLASS2_</li> <li>E_BIKE_CLASS3_</li> </ul> <p>This field is ignored as it's assumed the trail is accessible by hikers.</p> <ul> <li>HIKER_PEDESTRIAN_</li> </ul>"},{"location":"trails/#dropped-fields_1","title":"Dropped Fields","text":"<p>These fields are dropped as unnecessary for OSM. Manye only have a NULL value anyway, so useless.</p> <ul> <li>MOTOR_WATERCRAFT_</li> <li>NONMOTOR_WATERCRAFT_</li> <li>GIS_MILES</li> <li>Geometry Column</li> <li>TRAIL_TYPE</li> <li>TRAIL_CN</li> <li>BMP</li> <li>EMP</li> <li>SEGMENT_LENGTH</li> <li>ADMIN_ORG</li> <li>MANAGING_ORG</li> <li>SECURITY_ID</li> <li>ATTRIBUTESUBSET</li> <li>NATIONAL_TRAIL_DESIGNATION</li> <li>TRAIL_CLASS</li> <li>ACCESSIBILITY_STATUS</li> <li>TRAIL_SURFACE</li> <li>SURFACE_FIRMNESS</li> <li>TYPICAL_TRAIL_GRADE</li> <li>TYPICAL_TREAD_WIDTH</li> <li>MINIMUM_TRAIL_WIDTH</li> <li>TYPICAL_TREAD_CROSS_SLOPE</li> <li>SPECIAL_MGMT_AREA</li> <li>TERRA_BASE_SYMBOLOGY</li> <li>MVUM_SYMBOL</li> <li>TERRA_MOTORIZED</li> <li>SNOW_MOTORIZED</li> <li>WATER_MOTORIZED</li> <li>ALLOWED_TERRA_USE</li> <li>ALLOWED_SNOW_USE</li> </ul>"},{"location":"trails/#options","title":"Options","text":"<pre><code>-h, --help            show this help message and exit\n-v, --verbose         verbose output\n-i INFILE, --infile INFILE input data file\n-c, --convert         Convert feature to OSM feature\n-o OUTFILE, --outfile OUTFILE Output GeoJson file\n</code></pre>"},{"location":"usgs/","title":"US Topographical Data","text":""},{"location":"usgs/#us-topographical-trails","title":"US Topographical Trails","text":"<ul> <li>OBJECTID</li> <li>permanenti</li> <li>name</li> <li>namealtern</li> <li>trailnumbe</li> <li>trailnum_1</li> <li>sourcefeat</li> <li>sourcedata</li> <li>sourceda_1</li> <li>sourceorig</li> <li>loaddate</li> <li>trailtype</li> <li>hikerpedes</li> <li>bicycle</li> <li>packsaddle</li> <li>atv</li> <li>motorcycle</li> <li>ohvover50i</li> <li>snowshoe</li> <li>crosscount</li> <li>dogsled</li> <li>snowmobile</li> <li>nonmotoriz</li> <li>motorizedw</li> <li>primarytra</li> <li>nationaltr</li> <li>lengthmile</li> <li>networklen</li> <li>SHAPE_Leng</li> </ul>"},{"location":"usgs/#us-topographical-highways","title":"US Topographical Highways","text":"<ul> <li>OBJECTID</li> <li>permanent_</li> <li>source_fea</li> <li>source_dat</li> <li>source_d_1</li> <li>source_ori</li> <li>loaddate</li> <li>interstate</li> <li>us_route</li> <li>state_rout</li> <li>county_rou</li> <li>federal_la</li> <li>stco_fipsc</li> <li>tnmfrc</li> <li>name</li> <li>mtfcc_code</li> <li>intersta_1</li> <li>intersta_2</li> <li>intersta_3</li> <li>us_route_a</li> <li>us_route_b</li> <li>us_route_c</li> <li>state_ro_1</li> <li>state_ro_2</li> <li>state_ro_3</li> <li>SHAPE_Leng</li> </ul>"},{"location":"utilities/","title":"Utility Programs","text":"<p>To conflate external datasets with OSM, the external data needs to be converted to the OSM tagging schema. Otherwise comparing tags gets very convoluted. Since every dataset uses a different schema, included are a few utility programs for converting external datasets. Currently the only datatsets are for highways. These datasets are available from the USDA, and have an appropriate license to use with OpenStreetMap. Indeed, some of this data has already been imported. The files are available from the  FSGeodata Clearinghouse</p> <p>Most of the fields in the dataset aren't needed for OSM, only the reference number if it has one, and the name. Most of these highways are already in OSM, but it's a bit of a mess, and mostly invalidated. Most of the problems are related to the TIGER import in 2007. So the goal of these utilities is to add in the TIGER fixup work by updating or adding the name and a reference number. These utilities prepare the dataset for conflation.</p> <p>There are other fields in the datasets we might want, like surface type, is it 4wd only, etc... but often the OSM data is more up to date. And to really get that right, you need to ground truth it.</p>"},{"location":"utilities/#mvumpy","title":"mvum.py","text":"<p>This converts the Motor Vehicle Use Map(MVUM) dataset that contains data on highways more suitable for offroad vehicles. Some require specialized offroad vehicles like a UTV or ATV. The data in OSM for these roads is really poor. Often the reference number is wrong, or lacks the suffix. We assume the USDA data is correct when it comes to name and reference number, and this will get handled later by conflation.</p>"},{"location":"utilities/#roadcorepy","title":"roadcore.py","text":"<p>This converts the Road Core vehicle map. This contains data on all highways in a national forest. It's similar to the MVUM dataset.</p>"},{"location":"utilities/#trailspy","title":"trails.py","text":"<p>This converts the NPSPublish Trail dataset. These are hiking trails not open to motor vehicles. Currently much of this dataset has empty fields, but the trail name and reference number is useful. This utility is to support the OpenStreetMap US Trails Initiative.</p>"},{"location":"utilities/#usgspy","title":"usgs.py","text":"<p>This converts the raw data used to print Topographical maps in the US. This obviously is a direct source when it comes to names if you want to be accurate. Although things do change over time, so you still have to validate it all. The files are available from the National Map. I use the Shapefiles, as the different categories are in separate files inside the zip. Each one covers a 7.5 quad square on a topo map. These have to be merged together into a single file to be practical.</p>"},{"location":"utilities/#osmhighwayspy","title":"osmhighways.py","text":"<p>On the OSM wiki, there is a list of incorrect tagging for forest highway names. Basically the name shouldn't be something like \"Forest Service Road 123.4A\". That's actually a reference number, not a name. This is primarily a problem with existing OSM data. These would all have to get manually fixed when validating in JOSM, so this program automates the process so you only have to validate, and not edit the feature. This also extracts only highway linestrings, so is used to create the OSM dataset for conflation. Since the other external datasets also correctly use name, ref, and ref:usfs, this simplifys conflation. Otherwise the algorithm would get very complicated and hard to maintain.</p>"},{"location":"utilities/#geojson2polypy","title":"geojson2poly.py","text":"<p>This is a very simple utility to convert a GeoJson boundary Multipolygon into an Osmosis poly file. This can be used with osmium, or osmconvert to make data extracts.</p>"},{"location":"validating/","title":"Validating The Conflation","text":"<p>Every feature must be validated before it can be uploaded to OpenStreetMap. OSM discourages machine editing without human intervention. This software doesn't make any geometry changes, just tags. At it's core, conflation is just merging tags between datasets to avoid tedious cut &amp; paste. But this still needs to be validated as bugs and inconsistencies in the datasets can sneak in.</p> <p></p>"},{"location":"validating/#my-process","title":"My Process","text":"<p>I have the two datasets that I was conflating loaded as layers in JOSM so I can check the original sources easily if needed. Since these are remote highways and trails, I use the USGS topographical basemap in JOSM. Course that can also lead to confusion as sometimes the reference number in the basemap has been truncated. When in doubt, I reference the latest dataset from the national forest service, which is the most correct. That dataset is in vtpk format, so doesn't work in JOSM, so I run it in QGIS.</p>"},{"location":"validating/#debug-tags","title":"Debug Tags","text":"<p>Currently a few tags are added to each feature to aid in validating and debugging the conflation algorythm. These should obviously be removed before uploading to OSM. They'll be removed at a future date after more validation of the software. These are:</p> <ul> <li>hits - The number of matching tags in a feature</li> <li>ratio - The ratio for name matching if not 100%</li> <li>dist -  The distance between features</li> <li>angle - The angle between two features</li> <li>slope - The slope between two features</li> </ul> <p>If a feature has 3 hits, that's probably an exact match. One hit is sufficient if there are no other nearby highways or trails. Two hits usually means the name and reference number matched. If a highway only contains highway=path, but the distance is very small, and there are no other nearby features, it's assumed to be a match. Modifying the current thresholds for distance, angle, and slope changes the results, so having these as temporary tags in the feature is useful when validating conflation results.</p>"},{"location":"validating/#missed-highway","title":"Missed Highway","text":"<p>Sometimes you see an external trail or highway that is not in the conflated data. This is because that highway is not currently in OSM, so we ignore it since we're focused on existing features. When you have the external dataset and OSM loaded in JOSM as layers, this screenshot shows the other layers under the conflated data and show as black lines. It's easy to see which layer it came from by toggling the layers on and off.</p> <p></p> <p>The conflation software can also produce a data file of highways in the MVUM dataset that aren't in OSM. Importing those is a different process, so not discussed here. The conflation software can also produce a data file of all the trails in OSM that are not in any official sources.</p>"},{"location":"validating/#missing-segments","title":"Missing Segments","text":"<p>Sometimes the external datset has missing segments, where OSM has the entire highway. You can see in this screenshot of an MVUM highway on top of an OSM basemap. The MVUM highway is missing the middle segment. The conflation software sucessfully merges the tags from the external dataset to the complete OSM highway feature.</p> <p></p> <p>Currently any features that are an exact match between the external dataset and OSM are not in the conflated output to reduce the data that needs to be validated. If you have OSM loaded into a layer in JOSM, the matched segments will be black lines.</p>"},{"location":"validating/#reference-numbers-dont-match","title":"Reference Numbers Don't Match","text":"<p>If you are using the USGS topographical basemap, you can't depend on it for the official reference number. Often the reference numbers in the basemap are truncated, so you may think there is a problem. The reference number in the MVUM dataset is the correct one.</p> <p></p> <p>The other issue with reference numbers is also related to them being truncated. Older versions of the external datasets are often missing the .1 suffix. All the newer versions of the MVUM dataset and the USGS topographical maps do have the full reference number with the .1 appended. Depending on when the data was imported into OSM, it may be lacking the .1 suffix.</p>"},{"location":"validating/#proper-abbreviation","title":"Proper Abbreviation","text":"<p>The community accepted abbreviation for Forest Service Road is FR. The MVUM dataset of course lacks this abbreviation, but it's added when converting the MVUM dataset to OSM. In OSM there is a wide variety of variations on this, some examples are usfs *, *usfsr, FD, FS, etc... The conversion process also looks for these and makes them consistent with OSM by using only FR. If they are converteds, the existing value in OSM is renamed to old_ref, so when validating the conflated data, you'll see both, and the old value can be deleted.</p>"},{"location":"validating/#proper-tag","title":"Proper Tag","text":"<p>One of the wonderful things about the flexibility of thw data schema to support multiple values for the same feature. An MVUM highway will of course have a reference number, but it may also have a county reference number.</p> <p>Based on a long discussion on the OSM Tagging email list a few years ago, the consensus was to use ref for the county reference number, and ref:usfs for the forest service number. The OSM carto will display anything with a ref* tag. The conflation process also looks for a forest service reference number under a ref_ tag, and changes it to use ref:usfs. Both versions are displayed in the conflated data, you can delete the older one under ref.</p>"},{"location":"validating/#geometry-matching","title":"Geometry Matching","text":"<p>If the distance, the slope, and the angle are all zero, that's an exact match of geometries. Usually these highways segments were imported from the same external dataset I'm conflating, so identical other than lacking metadata. These an easy to validate since we have high confidence the external feature matches the OSM feature.</p> <p>Sometimes though you get a similar geometry, but they are parallel. This happens for highways that were traced off of satelloite imagery as the offsets vary. You can see in this screenshot that the conflation software sucessfully made the match, and the merged the tags into the OSM feature, which currently is only highway=track. At some point it'd be good to go back through and fix the geometry, but for now we're just focused on improving the tags.</p> <p></p> <p>When highways are traced from satellite imagery, sometimes they don't match the geometry in the external dataset. While too much differences in geometry can lead to false positives, we don't want to only identify an exact match. There are steering paramaters on how much difference is acceptable.</p> <p></p>"},{"location":"validating/#splitting-forks","title":"Splitting Forks","text":"<p>While this project is focused on conflating metadata, and not making geometry changes, sometimes when validating the conflation results I come across problems. Usually in OSM these are just tagged with highway=track, and were obviously traced from satellite imagery. Unfortunately without another dataset to reference, sometimes the highway continues on the wrong branch of the fork. This obviously gets flagged by the conflation process, but needs to be fixed manually.</p> <p></p> <p>I try to fix these as I come across them as I'm validating the conflation results. Since conflation is not fast, I have time between conflation runs as I improve the algorythm. Fixing these takes a few minutes per feature, and you have to be careful you don't break navigation. I work on these in JOSM while waiting for other tasks to finish. Since I can regenerate all the OSM data extracts for conflation, this will improve the results in the future. If I add the tags too, when conflating later it'll be a perfect match, so won't be in the results.</p> <p>To fix this I select the node at the fork, and split the highway that is wrong. Then I have to manually cut &amp; paste the tags to the correct branches of the fork. Sometimes I'll combine the ways for the newly fixed highways, but that is optional.</p>"},{"location":"wiki_redirect/","title":"OSM Merge","text":"<p>Please see the docs page at: https://hotosm.github.io/osm-merge/</p>"},{"location":"zion/","title":"Analyzing Zion National Park Trails","text":"<p>As an aid to debugging my conflation software, I decided to use Zion National Park trail data. This involved two external datasets, USGS vector topographical maps and the National Park Service trails dataset.The Topographical maps are in ShapeFile format, the NPS trails is in GeoJson.</p> <p>The topographical dataset has many more attributes than the NPS dataset. For example, the topo dataset contains access information, which is one of the goals of the Trail Access Project. One of the details I noticed was having a value of designated instead of yes if the trail is in an official source. There are multiple access types, horse, bicycles, etc... having them be no might be useless data as it could be assumed if the access is allowed.</p> <pre><code>\"properties\": {\n    \"highway\": \"path\",\n    \"source\": \"National Park Service\",\n    \"bicycle\": \"no\",\n    \"atv\": \"no\",\n    \"horse\": \"designated\",\n    \"motorcycle\": \"no\",\n    \"snowmobile\": \"no\"\n    },\n</code></pre>"},{"location":"zion/#conflating-with-openstreetmap","title":"Conflating with OpenStreetMap","text":"<p>One big difference is that the OpenStreetMap dataset has many more features tagged with highway than the other datasets. OSM has mucn more detail, campground loop roads, service roads, </p> <p>Topo Trails  Coalpits Wash Trail (official) Dalton Wash Trail (BLM ?) Huber Wash Trail (not sure) Left Fork North Creek Trail aka Subway (official)</p> <p>The Subway (Bottom) in Topo and Left Fork North Creek Trail in OSM</p> <p>Pa'rus Trail is same in topo and nps, not in OSM.</p> <p>Deertrap Mountain Trail, or Cable Mountain.</p> <p>nps:COMMENT=062904-GPSed for cultural projects coverage nps:EDIT_DATE=082004 nps:ED_COMMENT=063004-removed spikes from arc nps:MILES=0.182262</p>"}]}